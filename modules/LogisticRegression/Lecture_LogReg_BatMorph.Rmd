---
title: "Logistic Regression Lecture"
author: Derek H. Ogle
layout: page
css: "/css/modules.css"
output:
  html_document:
    fig_height: 3.5
    fig_width: 3.5
    self_contained: false
---

```{r echo=FALSE, eval=FALSE, warning=FALSE}
## Renders an appropriate HTML file and moves to CE directory
source("rhelpers/rhelpers.R")
modHTML("modules/LogisticRegression/Lecture_LogReg_BatMorph")
```
```{r echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
library(NCStats)
knitr::opts_chunk$set(prompt=TRUE,comment="")
options(show.signif.stars=FALSE)
```

----

## Background
Researchers measured (among other things) the canine tooth height (cm) from two subspecies of Hoary bats (*Lasiurus cinereus cinereus* and *Lasiurus cinereus semotus*) found in Hawaii. Their primary question was to determine if canine tooth height differed between the subspecies and, more importantly to them, can canine tooth height be used to predict the subspecies of bat. In this lecture we will focuse on their primary goal -- can canine tooth height be used to predict the subspecies of bat. With this, 

* What are the response and explanatory variables?[^Vars]
* What type of analysis should be used?[^Method]

The data are loaded into R below. For class demonstration purposes only, the data.frame was reduced to only the two variables of interest. In addition, the `canine` variable was converted from cm to mm so that the slope would be more usefully interpreted.[^cm2mm] Neither of these decisions is required for a logistic regression.
```{r}
bat <- read.csv("https://raw.githubusercontent.com/droglenc/NCData/master/Batmorph.csv")
bat <- bat[,c("subsp","canine")]  # for class demo purposes only
bat$canine <- bat$canine*10      # convert cm to mm
xlbl <- "Canine Tooth Height (mm)"
ylbl <- "Subspecies Code"
str(bat)
```

<div class="alert alert-info">
<ul>
  <li>Note above that `cinereus` is listed as the first level for `subsp` and, thus, will be coded with a 0 (and `semotus`) will be coded with a 1. Recall that R lists levels alphabetically and codes the first item as 0. This ordering is important in the discussion that follows.</li>
</ul>
</div>

Before beginning this analysis, I like to examine the data to see if it is going to be reasonable to distinguish between the two subspecies based on canine tooth height. The histograms show some overlap but also considerable separation between the two subspecies. Thus, it may be reasonable to separate the two subspecies for many canine tooth heights.[^hist]

```{r echo=-1, fig.height=5}
par(mar=c(3,3,1,1),mgp=c(1.9,0.5,0),tcl=-0.2,cex=0.8,cex.main=0.8)
hist(canine~subsp,data=bat,w=0.1,xlim=c(2.6,3.8),ymax=20,xlab=xlbl,nrow=2,ncol=1)
```

<br>

## Preparing to Model

All models that we have fit in class have been linear (at least after transformation) and represented the mean of the response variable (recall that all models had &mu;<sub>Y</sub>) on the right-hand-side. At first this seems like an issue in this case because the response variable is categorical. How do you compute the mean of "words"?

Recall that behind the scenes R has converted the levels into numbers -- *cinereus* as a 0 and *semotus* as a 1. Now suppose that you have five hoary bats and two are *cinereus* and three are *semotus*. Behind the scenes this is the same as having two 0s and three 1s. The mean of these values is thus 3 (the sum of the 0s and 1s) divided by 5 (total number of numbers) or 0.6. This is ALSO precisely the propotion of *semotus* of those five hoary bats (i.e., 3 *semotus* divided by 5 total bats).

<div class="alert alert-info">
<ul>
  <li>The mean of a binary categorical variable is exactly the same as the proprtion of individuals in the second level of the variable.</li>
  <li>The second level of a binary categorical variable is generically considered a "success" (and, thus, the first level is a "failure").</li>
</ul>
</div>

One way to visualize logistic regression data is to plot the categorical response (but as numbers) and the quantitative explanatory (see below). Because of the nature of the categorical data there will be many points plotted on top of each other. Thus, points are plotted with transparency such that darker "points" actually represent more points. In the plot below you can see that canine tooth heights are always *semotus* until about 3 mm where some *cinereus* appear, but then they are all *cinereus* after about 3.4 mm.

```{r echo=FALSE,fig.width=5}
par(mar=c(3.5,3.5,1.1,3.5),mgp=c(1.9,0.5,0),tcl=-0.2)
plotBinResp(subsp~canine,data=bat,breaks=seq(2.6,3.8,0.1),xlim=c(2.6,3.8),
            xlab=xlbl,ylab=ylbl,plot.p=FALSE)
```

This plot can be modified by thinking of narrow vertical "windows" (dashed lines below). The mean of the points within each of these windows is computed and plotted in the center of the window with a "blue plus sign." Recall that these means are also the proportions of "successes"; thus, these blue plusses also represent the proportion of *semotus* within each window. In the plot below, the first four "windows" have 100% *semotus*, then the percent that are *semotus* drops until it is 0% in the last three "windows." 

```{r echo=FALSE,fig.width=5}
par(mar=c(3.5,3.5,1.1,3.5),mgp=c(1.9,0.5,0),tcl=-0.2)
plotBinResp(subsp~canine,data=bat,breaks=seq(2.6,3.8,0.1),xlim=c(2.6,3.8),
            xlab=xlbl,ylab=ylbl)
abline(v=seq(2.5,3.9,0.1),lty=2,col="gray70")
```

Logistic regression tries to fit a model that best represents the blue plusses (i.e., the means as with other models, but remembering that these are also proportions). However, the blue plusses are clearly not linear. What do we try to do when the data are non-linear?

<div class="alert alert-info">
<ul>
  <li>A logistic regression model attempts to fit a linear model to the proportion of "successes" and the quantitative explanatory variable.</li>
  <li>The proportion of "successes" is very rarely linear. The easiest way to visualize this is to realize that the propotion of "successes" must be between 0 and 1. Thus, the line must bend at the edges to vertically stay between 0 and 1.</li>
</ul>
</div>

<br>

## Transformations
The transformation required to linearize the proportion of "successes" is a two-step process. These two steps are discusses separately below.

### Odds
The proportion of successes in the ith "window" of the plot above is p<sub>i</sub>. For example, the fifth "window" had 80% *semotus*, so p<sub>5</sub>=0.80. Similarly the seventh "window" had 40% *semotus* so p<sub>7</sub>=0.40. The p<sub>i</sub> are interpreted as the probability of "success." For example, the probability of being a *semotus* in the fourth window is 0.8. Thus, 1-p<sub>i</sub> is the probability of "failure." For example, the probability of being a *cinereus* (i.e., not a *semotus*) in the fourth "window" is 1-p<sub>4</sub>=1-0.8=0.2.

The odds of a "success" are defined as $\frac{\text{p}_i}{1-\text{p}_i}$. This is interpreted as the ratio of the probability of success to the probability of failure. If the odds are 1 then there are equal probabilities of "success" and "failure" (think of flipping a fair coin). If the odds are greater than 1 then there is a higher probability of a "success" and if the odds are less than 1 then there is a higher probability of a "failure."

In the examples above, the odds for the fourth "window" are $\frac{0.8}{1-0.8}$=$\frac{0.8}{0.2}$=4. Thus, in the fourth "window" it is four times more likely for the bat to be a *semotus* than a *cinereus*. In contrast, the odds for the seventh "window" are $\frac{0.4}{1-0.6}$=$\frac{0.4}{0.6}$=0.67. Thus, in the seventh "window" it is 0.67 times as likely or the bat to be a *semotus* than a *cinereus*. It is often easier when describing odds that are less than 1 to flip them. For example, the inverse of the odds for the seventh "window" is $\frac{1}{0.67}$=1.5, which can be interpreted as the probability that the bat is a *cinereus* (i.e., a failure) is 1.5 times the probability that it is a *semotus*.

Odds are useful in general, but they are particularly useful here as the first step in transforming the non-linear probabilities. Consider the following table.

| p<sub>i</sub> | odds <sub>i</sub> |
|--------------:|------------------:|
|        0.9999 |              9999 |
|         0.999 |               999 |
|          0.99 |                99 |
|           0.9 |                 9 |
|          0.75 |                 3 |
|           0.5 |                 1 |
|          0.25 |            0.3333 |
|           0.1 |            0.1111 |
|          0.01 |            0.0101 |
|         0.001 |            0.0010 |
|        0.0001 |            0.0001 |

This implies that as the probabilities (p) get closer and closer to 1 then the odds go to positive infinity. On the other sise, as the probabilities get closer and closer to 0 then the odds also get closer and closer to zero. Thus, for example, converting the probabilities that were the blue plusses in the plot above to odds produces the plot below.

```{r echo=FALSE,fig.width=5}
par(mar=c(3.5,3.5,1.1,3.5),mgp=c(1.9,0.5,0),tcl=-0.2)
ps <- c(0.995,0.99,0.95,0.9,0.8,0.53,0.4,0.19,0.05,0.01,0.005,0.001)
odds <- ps/(1-ps)
logits <- log(odds)
xs <- seq(2.6,3.7,0.1) + 0.05
tmp <- data.frame(xs,ps,odds,logits)

plot(odds~xs,data=tmp,xlab=xlbl,ylab="Odds of 'semotus'",pch=3,col="blue",xlim=c(2.6,3.8))
```

### Log Odds or Logits

At first glance, computing the odds does not seem to be much of an improvement because the plot (above) is still clearly non-linear. However, it now closely resembles an exponential function that you are familiar with. How do you think you should transform this?[^logodds]

```{r echo=FALSE,fig.width=5}
par(mar=c(3.5,3.5,1.1,3.5),mgp=c(1.9,0.5,0),tcl=-0.2)
plot(logits~xs,data=tmp,xlab=xlbl,ylab="log Odds of 'semotus'",pch=3,col="blue",xlim=c(2.6,3.8))
```

Thus, a linear relationship is observed if the probabilities are transformed to the log odds. Thus, the model that will be fit is

\[ \text{log}\left(\frac{\text{p}}{1-\text{p}}\right) = \alpha+\beta X \]


## Model Fitting
### Using `glm()`
Fitting this linear model in R requires using `glm()` rather than `lm()`. The first two arguments to `glm()` are the same as what you would use for `lm()` -- i.e., a formulat of the form `response~explanatory` and the data frame given to `data=`. However, to force `glm()` to fit a logistic regression to the log odds transformed probabilities, you must also include `family=binomial`. The result is saved to an object per usual

```{r}
glm1 <- glm(subsp~canine,data=bat,family=binomial)
```

### Parameter Interpretations and Tests
Parameter estimates are extracted from the object saved with `glm()` with `coef()` and `confint()` as with `lm()`. These results are organized as before -- i.e., values associated with the y-intercept are in the `(Intercept)` row and those associated with the slope are in the row labeled with the quantitative explanatory variable (i.e., `canine` in this example).

```{r}
( cfs <- cbind(Ests=coef(glm1),confint(glm1)) )
```

As with linear models, interpretation of the slope is most important. In logistic regression, the slope measures how much the LOG ODDS change for a one unit increase in the explanatory variable. Thus, in this case, the LOG ODDS of being a *semotus* decrease by betwen `r formatC(-cfs[2,3],format="f",digits=1)` and `r formatC(-cfs[2,2],format="f",digits=1)` for every 1 mm increase in canine tooth height. This is visualized below for an increase from 2.6 to 3.6 mm of canine tooth height.

| point | tooth height | log odds |
|:-----:|--------------:|------------------:|
|   1   |        2.6 | 35.51574-11.11193&times;2.6 =  6.624722 |
|   2   |        3.6 | 35.51574-11.11193&times;3.6 = -4.487208 |
| DIFF  |        1.0 | -4.487208 - 6.624722 = -11.11193 (&beta;) |

```{recho=FALSE, fig.height=4.5, fig.width=5}
par(mar=c(3,3,0.5,0.5),mgp=c(1.9,0.5,0),tcl=-0.2)
xs <- seq(2.6,3.8,length.out=99)
logodds <- predict(glm1,data.frame(canine=xs))
tmp <- data.frame(x=xs,logodds=logodds,odds=exp(logodds))
plot(logodds~xs,data=tmp,type="l",lwd=2,col="red",xlim=c(2.6,3.8),xlab=xlbl,
     ylab="log Odds of 'semotus'")
xs2 <- c(2.6,3.6)
logodds2 <- predict(glm1,data.frame(canine=xs2))
points(xs2,logodds2,pch=19)
arrows(xs2[1],logodds2[1],xs2[2],logodds2[1],col="blue",length=0.15,angle=20)
arrows(xs2[2],logodds2[1],xs2[2],logodds2[2],col="blue",length=0.15,angle=20)
text(mean(xs2),logodds2[1],"1 mm increase",pos=1)
text(xs2[2],logodds2[1],"    log Odds decrease by 11.11193",pos=4,srt=-90)
```

It is very hard to interpret results on the log scale. Thus, the slope is often back-transformed by raising it to the power of e (i.e., e<sup>&beta;</sup>). The back-transformed slope provides a MULTIPLE for how the ODDS change for a one unit increase in the explanatory variale. For examle, all of the parameter estimates are back-transformed as shown below.
```{r}
exp(cfs)
```
```{r echo=FALSE}
cfs1 <- exp(cfs)
```
The back-transformed slope then means that the odds of being a *semotus* are between `r formatC(cfs1[2,2],format="f",digits=7)` and `r formatC(cfs1[2,3],format="f",digits=7)` TIMES the odds of being a *cinereus* when the canine tooth height increases by 1 mm. In other words, if the canine tooth height increases by 1 mm then it becomes much more unlikely that the bat is a *semotus*. This is visualized below for an increase from 2.6 to 3.6 mm of canine tooth height.

| point | tooth height | odds (from log(odds) above) |
|:-----:|--------------:|------------------:|
|   1   |        2.6 | e<sup>6.624722</sup> = 0.01125202 |
|   2   |        3.6 | e<sup>-4.487208</sup> = -4.487208 |
| RATIO  |       --- | $\frac{0.01125202}{753.4947}$ = 0.00001493 (e<sup>&beta;</sup>) |

```{recho=FALSE, fig.height=4.5, fig.width=5}
par(mar=c(3,3,0.5,0.5),mgp=c(1.9,0.5,0),tcl=-0.2)
plot(odds~xs,data=tmp,type="l",lwd=2,col="red",xlim=c(2.6,3.8),xlab=xlbl,
     ylab="Odds of 'semotus'")
odds2 <- exp(predict(glm1,data.frame(canine=xs2)))
points(xs2,odds2,pch=19)
arrows(xs2[1],odds2[1],xs2[2],odds2[1],col="blue",length=0.15,angle=20)
arrows(xs2[2],odds2[1],xs2[2],odds2[2],col="blue",length=0.15,angle=20)
text(mean(xs2),odds2[1],"1 mm increase",pos=1)
text(xs2[2],odds2[1],"    Odds are 0.00001493 times",pos=4,srt=-90)
```






```{r}
fitPlot(glm1,breaks=seq(2.6,3.8,0.1),xlim=c(2.6,3.8),xlab=xlbl,ylab=ylbl)
```


`summary()`.  Thre rest of the output from `summary()` can be ignored. Confidence
```{r}
summary(glm1)
```


----

## Footnotes
[^Vars]: The researchers are trying to predict subspecies so it is the response variable. Thus, canine tooth height is the explanatory variable.
[^Method]: The subspeces response variable is categorical (and binomial) and the canine tooth height explanatory variable is quantitative. Thus, this question requires a (binary) logistic regression.
[^cm2mm]: The range of canine tooth heights was less than 1cm. Thus, when interpreting the slope a "1cm increase in canine tooth height" was not realistic. Thus, this variable was multiplied by 10 to convert the cm to mm such that a slope would be for a "1mm increase in canine tooth height" and would thus would not be a larger increase then the range of the data.
[^hist]: There are several arguments used in this `hist()` that you may not have seen before. The `w=` controls how wide the bins are, `ymax=` sets a common maximum value for the two y-axes, `ncol=` sets how many columns the plots will be placed in, and `nrow=` sets how many rows the plots will be placed in.
[^logodds]: As this does follow an exponential function then, from our work in the SLR module, only the response variable should be log-transformed. Thus, the log of the odds should be computed.
