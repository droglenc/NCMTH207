---
layout: page
title: "Logistic Regression Lecture"
author: Derek H. Ogle
css: "/css/modules.css"
---


<hr />
<div id="background" class="section level2">
<h2>Background</h2>
<p><img src="../zimgs/bat2.jpg" alt="Hoary Bat" class="img-right"> Researchers measured (among other things) the canine tooth height (cm) from two subspecies of <a href="https://en.wikipedia.org/wiki/Hoary_bat">Hoary bats</a> (<em>Lasiurus cinereus cinereus</em> and <em>Lasiurus cinereus semotus</em>) found in Hawaii. Their primary question was to determine if canine tooth height (hereafter, just tooth height) differed between subspecies and, more importantly to them, can tooth height be used to predict the subspecies of bat. In this lecture we will focuse on their primary goal – can tooth height be used to predict the subspecies of bat. With this,</p>
<ul>
<li>What are the response and explanatory variables?<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></li>
<li>What type of analysis should be used?<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></li>
</ul>
<p>The data are loaded into R below. For class demonstration purposes only, the data.frame was reduced to only the two variables of interest. In addition, <code>canine</code> was converted from cm to mm so that the slope would be more meaningful.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Neither of these decisions is required for a logistic regression.</p>
<pre class="r"><code>&gt; bat &lt;- read.csv(&quot;https://raw.githubusercontent.com/droglenc/NCData/master/Batmorph.csv&quot;)
&gt; bat &lt;- bat[,c(&quot;subsp&quot;,&quot;canine&quot;)]   # for class demo purposes only
&gt; bat$canine &lt;- bat$canine*10        # convert cm to mm
&gt; xlbl &lt;- &quot;Canine Tooth Height (mm)&quot;
&gt; ylbl &lt;- &quot;Subspecies Code&quot;
&gt; str(bat)</code></pre>
<pre><code>&#39;data.frame&#39;:   118 obs. of  2 variables:
 $ subsp : Factor w/ 2 levels &quot;cinereus&quot;,&quot;semotus&quot;: 2 2 2 2 2 2 2 2 2 2 ...
 $ canine: num  3.26 3.08 2.91 2.87 3.01 3.05 2.77 3.13 2.89 2.93 ...</code></pre>
<div class="alert alert-info">
<ul>
<li>
Recall that R lists levels alphabetically and codes the first item as 0. Note then that <code>cinereus</code> is listed as the first level for <code>subsp</code> and, thus, will be coded with a 0 (and <code>semotus</code> will be coded with a 1). This ordering is important below as R will consider the ‘1’ group to be a “success.”
</li>
</ul>
</div>
<p>Before beginning this analysis, I like to examine the data to see if it is reasonable to distinguish between the two subspecies based on tooth height. The histograms below show some overlap but also considerable separation between the subspecies. Thus, it may be reasonable to separate the two subspecies for many tooth heights.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<pre class="r"><code>&gt; hist(canine~subsp,data=bat,w=0.1,xlim=c(2.6,3.8),ymax=20,xlab=xlbl,nrow=2,ncol=1)</code></pre>
<p><img src="Lecture_LogReg_BatMorph_files/figure-html/unnamed-chunk-4-1.png" width="336" /></p>
<p><br></p>
</div>
<div id="preparing-to-model" class="section level2">
<h2>Preparing to Model</h2>
<p>All models that we have fit in class have been linear (at least after transformation) and represented the mean of the response variable (recall that all models had μ<sub>Y</sub>) on the right-hand-side. This may seem like an issue here because the response variable is categorical. How do you compute the mean of “words”? However, recall that behind the scenes R has converted the levels into numbers – <em>cinereus</em> as a 0 and <em>semotus</em> as a 1.</p>
<p>Suppose that you have five hoary bats and two are <em>cinereus</em> and three are <em>semotus</em>. Behind the scenes this is the same as having two 0s and three 1s. The mean of these values is thus <span class="math inline">\(\frac{3 \text{ (sum of 0s and 1s)}}{5 \text{ (total number of bats)}}\)</span>=0.6. This is ALSO precisely the propotion of those five hoary bats that are <em>semotus</em> (i.e., <span class="math inline">\(\frac{3 \text{ (number of semotus)}}{5 \text{ (total number of bats)}}\)</span>=0.6).</p>
<div class="alert alert-info">
<ul>
<li>
The mean of a binary categorical variable is exactly the same as the proprtion of individuals in the second level of the variable.
</li>
<li>
The second level of a binary categorical variable is generically considered a “success” (and, thus, the first level is a “failure”).
</li>
</ul>
</div>
<p>One way to visualize logistic regression data is to plot the categorical response (but as their numberic codes) and the quantitative explanatory (see below). Because of the nature of categorical data there will be many points plotted on top of each other. Thus, points are plotted with transparency such that darker “points” actually represent more points. In the plot below you can see that tooth heights are always <em>semotus</em> until about 3.0 mm, there is a mix of <em>semotus</em> and <em>cinereus</em> between 3.0 and about 3.4 mm (where there are more bats), and there is all <em>cinereus</em> after about 3.4 mm.</p>
<p><img src="Lecture_LogReg_BatMorph_files/figure-html/unnamed-chunk-5-1.png" width="480" /></p>
<p>This plot can be modified by thinking of narrow vertical “windows” (dashed lines below). The mean of the points within each of these windows is computed and plotted in the center of the window with a “blue plus sign.” Recall that these means are also the proportions of “successes”; thus, these blue plusses also represent the proportion of <em>semotus</em> within each window. In the plot below, the first four “windows” up to about 3.0 mm have 100% <em>semotus</em>, the percent of <em>semotus</em> declines in windows between about 3.0 and 3.4 mm, and the percent that are <em>semotus</em> is 0% in the “windows” larger than 3.4 mm.</p>
<p><img src="Lecture_LogReg_BatMorph_files/figure-html/unnamed-chunk-6-1.png" width="480" /></p>
<p>Logistic regression tries to fit a model that best represents the blue plusses (i.e., the means as with other models, but remembering that these are also proportions of a categorical variable in logistic regression). However, the blue plusses are clearly not linear. What do we try to do when the data are non-linear?</p>
<div class="alert alert-info">
<ul>
<li>
A logistic regression model attempts to fit a linear model to the proportion of “successes” and the quantitative explanatory variable.
</li>
<li>
The proportion of “successes” is very rarely linear. The easiest way to think about this is to realize that the propotion of “successes” must be between 0 and 1. Thus, the line must bend at the edges to vertically stay between 0 and 1.
</li>
<li>
The bending at the edges to stay between 0 and 1 leads to an “S-curve” that is often referred to as a “logistic curve” (think of population growth from your ecology class), which is where the name “logistic regresion” comes from.
</li>
</ul>
</div>
<p><br></p>
</div>
<div id="transformations" class="section level2">
<h2>Transformations</h2>
<p>The transformation required to linearize the proportion of “successes” is a two-step process, which are discussed separately below.</p>
<div id="odds" class="section level3">
<h3>Odds</h3>
<p>The proportion of successes in a “window” of the plot above is labelled as p. For example, the fifth “window” had 80% <em>semotus</em>, so p for that “window” is 0.80. Similarly the seventh “window” had 40% <em>semotus</em> so p for that window is 0.40. These p are interpreted as the <strong>probability</strong> of “success” in logistic regression. For example, the probability of being a <em>semotus</em> in the fourth window is 0.8. Thus, 1-p is the probability of “failure.” For example, the probability of being a <em>cinereus</em> (i.e., not a <em>semotus</em>) in the fourth “window” is 1-0.8=0.2.</p>
<p>The odds of a “success” are defined as <span class="math inline">\(\frac{\text{p}}{1-\text{p}}\)</span>. This is interpreted as the ratio of the probability of success to the probability of failure. If the odds are 1 then there are equal probabilities of “success” and “failure” (think of flipping a fair coin). If the odds are greater than 1 then there is a higher probability of a “success” and if the odds are less than 1 then there is a higher probability of a “failure.”</p>
<p>In the examples above, the odds for the fourth “window” are <span class="math inline">\(\frac{0.8}{1-0.8}\)</span>=<span class="math inline">\(\frac{0.8}{0.2}\)</span>=4. Thus, in the fourth “window” it is four times more likely for the bat to be a <em>semotus</em> than a <em>cinereus</em>. In contrast, the odds for the seventh “window” are <span class="math inline">\(\frac{0.4}{1-0.6}\)</span>=<span class="math inline">\(\frac{0.4}{0.6}\)</span>=0.67. Thus, in the seventh “window” it is 0.67 times as likely for the bat to be a <em>semotus</em> than a <em>cinereus</em>. It is often easier when describing odds that are less than 1 to flip them. For example, the inverse of the odds for the seventh “window” is <span class="math inline">\(\frac{1}{0.67}\)</span>=1.5, which can be interpreted as the probability that the bat is a <em>cinereus</em> (i.e., a failure) is 1.5 times the probability that it is a <em>semotus</em>.</p>
<p>Odds are useful in general, but they are particularly useful here as the first step in transforming the non-linear probabilities. Consider the following table.</p>
<table>
<thead>
<tr class="header">
<th>p</th>
<th>0.999</th>
<th>0.99</th>
<th>0.9</th>
<th>0.75</th>
<th>0.5</th>
<th>0.25</th>
<th>0.1</th>
<th>0.01</th>
<th>0.001</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>odds</td>
<td>999</td>
<td>99</td>
<td>9</td>
<td>3</td>
<td>1</td>
<td>0.3333</td>
<td>0.1111</td>
<td>0.0101</td>
<td>0.0010</td>
</tr>
</tbody>
</table>
<p>This implies that as the probabilities (p) get closer and closer to 1 then the odds go to positive infinity. On the other side, as the probabilities get closer and closer to 0 then the odds also get closer and closer to 0.</p>
<div class="alert alert-info">
<ul>
<li>
Probabilities are constrained to be between 0 and 1.
</li>
<li>
Odds are constrained to be between 0 and infinity.
</li>
<li>
Odds and probabilities are not synonyms, though they are sometimes used as such in everyday language (but not by statisticians). The probability of something happening explains how likely that that something is going to happen. The probability that a woman walks through of 0.6 means that 60% of the people that walk through the door will be women. The odds that a woman walks through the of 1.5 means that there will be 1.5 women walking through the door for every man that walks through the door. Odds and probabilities are related, but not equal.
</li>
</ul>
</div>
<p>Thus, converting the probabilities that were the blue plusses in the plot above to odds produces the plot below. At first glance, computing the odds does not seem to be much of an improvement because the plot of odds is still non-linear. However, it now resembles an exponential function that you are familiar with. How should you transform this?<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p><img src="Lecture_LogReg_BatMorph_files/figure-html/unnamed-chunk-7-1.png" width="480" /></p>
<div class="alert alert-info">
<ul>
<li>
Transforming the probabilities to odds changes the curve from logistic to exponential.
</li>
</ul>
</div>
</div>
<div id="log-odds-or-logits" class="section level3">
<h3>Log Odds or Logits</h3>
<p>The exponential form of the odds versus X function can be linearized by transforming the odds to log(odds), as shown below.</p>
<p><img src="Lecture_LogReg_BatMorph_files/figure-html/unnamed-chunk-8-1.png" width="480" /></p>
<p>Thus, a linear relationship is observed if the probabilities are transformed to log(odds). Thus, the model that will be fit in logistic regression is</p>
<p><span class="math display">\[ \text{log}\left(\frac{\text{p}}{1-\text{p}}\right) = \alpha+\beta X \]</span></p>
<p><br></p>
</div>
</div>
<div id="model-fitting" class="section level2">
<h2>Model Fitting</h2>
<div id="using-glm" class="section level3">
<h3>Using <code>glm()</code></h3>
<p>Fitting this linear model in R requires using <code>glm()</code> rather than <code>lm()</code>.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> The first two arguments to <code>glm()</code> are the same as for <code>lm()</code> – i.e., a formula of the form <code>response~explanatory</code> and the data frame in <code>data=</code>. However, to force <code>glm()</code> to fit a logistic regression to the log(odds)-transformed probabilities, you must also include <code>family=binomial</code>.</p>
<pre class="r"><code>&gt; glm1 &lt;- glm(subsp~canine,data=bat,family=binomial)</code></pre>
</div>
<div id="interpreting-the-slope-and-back-transformed-slope" class="section level3">
<h3>Interpreting the Slope and Back-Transformed Slope</h3>
<p>Parameter estimates are extracted from the <code>glm()</code> object with <code>coef()</code> and <code>confint()</code> as with <code>lm()</code>. These results are organized as before – i.e., values associated with the y-intercept are in the <code>(Intercept)</code> row and those associated with the slope are in the row labeled with the quantitative explanatory variable (i.e., <code>canine</code> in this example).</p>
<pre class="r"><code>&gt; ( cfs &lt;- cbind(Ests=coef(glm1),confint(glm1)) )</code></pre>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>                 Ests     2.5 %   97.5 %
(Intercept)  35.51574  24.21685 49.66132
canine      -11.11193 -15.52430 -7.58941</code></pre>
<p>As with linear models, interpretation of the slope is most important. In logistic regression, the slope measures how much the LOG ODDS change for a one unit increase in the explanatory variable. Thus, in this case, the LOG ODDS of being a <em>semotus</em> decrease by betwen 7.6 and 15.5 for every 1 mm increase in tooth height. This is visualized below for an increase from 2.6 to 3.6 mm of tooth height.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<table>
<thead>
<tr class="header">
<th align="center">point</th>
<th align="right">tooth height</th>
<th align="right">log (odds)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">2.6</td>
<td align="right">35.51574-11.11193×2.6 = 6.624722</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">3.6</td>
<td align="right">35.51574-11.11193×3.6 = -4.487208</td>
</tr>
<tr class="odd">
<td align="center">DIFF</td>
<td align="right">1.0</td>
<td align="right">-4.487208 - 6.624722 = -11.11193 (see that this is β)</td>
</tr>
</tbody>
</table>
<p><img src="Lecture_LogReg_BatMorph_files/figure-html/unnamed-chunk-11-1.png" width="432" /></p>
<p>It is hard to interpret results on the log scale. Thus, the slope is often back-transformed with e<sup>β</sup>. The back-transformed slope provides a MULTIPLE for how the ODDS change for a one unit increase in the explanatory variale. For examle, all of the parameter estimates are back-transformed as shown below.</p>
<pre class="r"><code>&gt; exp(cfs)</code></pre>
<pre><code>                    Ests        2.5 %       97.5 %
(Intercept) 2.656377e+15 3.290379e+10 3.695180e+21
canine      1.493306e-05 1.810853e-07 5.057793e-04</code></pre>
<p>The back-transformed slope is that the odds of being a <em>semotus</em> are between 0.0000002 and 0.0005058 TIMES the odds of being a <em>cinereus</em> when the tooth height increases by 1 mm. In other words, if the tooth height increases by 1 mm then it becomes much more unlikely that the bat is a <em>cinereus</em>. This is visualized below for an increase from 2.6 to 3.6 mm of tooth height.</p>
<table>
<thead>
<tr class="header">
<th align="center">point</th>
<th align="right">tooth height</th>
<th align="right">odds (from log(odds) above)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">2.6</td>
<td align="right">e<sup>6.624722</sup> = 0.01125202</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">3.6</td>
<td align="right">e<sup>-4.487208</sup> = -4.487208</td>
</tr>
<tr class="odd">
<td align="center">RATIO</td>
<td align="right">—</td>
<td align="right"><span class="math inline">\(\frac{0.01125202}{753.4947}\)</span> = 0.00001493 (see that this is e<sup>β</sup>)</td>
</tr>
</tbody>
</table>
<p><img src="Lecture_LogReg_BatMorph_files/figure-html/unnamed-chunk-14-1.png" width="432" /></p>
</div>
<div id="default-tests-for-the-slope" class="section level3">
<h3>Default Tests for the Slope</h3>
<p>As before, the parameter estimates, standard errors, and a test that the parameter is equal to zero (or not) is obtained by submitting the <code>glm()</code> object to <code>summary()</code>. As above, information for the slope is in the row labeled with the quantitative explanatory variable. The output from <code>summary()</code> below the “Coefficients:” table can be ignored.</p>
<pre class="r"><code>&gt; summary(glm1)</code></pre>
<pre><code>
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   35.516      6.428   5.525 3.29e-08
canine       -11.112      2.005  -5.543 2.97e-08

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 163.040  on 117  degrees of freedom
Residual deviance:  97.178  on 116  degrees of freedom
AIC: 101.18

Number of Fisher Scoring iterations: 5</code></pre>
<p>The p-value for the slope is a test of whether the slope is equal to 0 or not. This tests if a relationship (i.e., a non-zero slope) between the LOG ODDS and the explanatory variable exists. If the H<sub>0</sub> is not rejected then no relationship exists and the blue plusses in the previous plots would all be equal and represented by a flat line.</p>
<p>As usual, <code>fitPlot()</code> can be used to visualize these results. For logistic regression it is best to also include <code>breaks=</code>, which controls the number of vertical “windows” at which the proportion of successes is calculated (and, thus, how many blue plusses are included on the plot). Finding an appropriate number is usually a matter of trial-and-error, but usually around 10 is a good idea. In the code below <code>seq()</code> is used to create a sequence of numbers that starts at 2.6, ends at 3.8, and goes in steps of 0.1 (i.e., 2.6, 2.7, 2.8, …, 3.7, 3.8).</p>
<pre class="r"><code>&gt; fitPlot(glm1,breaks=seq(2.6,3.8,0.1),xlim=c(2.6,3.8),xlab=xlbl,ylab=ylbl)</code></pre>
<p><img src="Lecture_LogReg_BatMorph_files/figure-html/unnamed-chunk-16-1.png" width="480" /></p>
<p>There are no formal assumptions to check with logistic regresion. Rather, we simply assess whether the fitted model (red line above) fits the proportion of “successes” (the blue plusses) reasonably closely. The fit displayed above is quite good.</p>
<p><br></p>
</div>
</div>
<div id="predicting-odds-and-probabilities" class="section level2">
<h2>Predicting Odds and Probabilities</h2>
<p>The equation for the best-fit line from the results above is</p>
<p><span class="math display">\[ \text{log}\left(\frac{\text{p}}{1-\text{p}}\right) = 35.51574-11.11193\times\text{Height} \]</span></p>
<p>Thus, plugging a value for tooth height into this equation results in a predicted value of the <strong>log(odds)</strong>. For example, the predicted log(odds) for a tooth height of 3.1 mm is</p>
<p><span class="math display">\[ \text{log}\left(\frac{\text{p}}{1-\text{p}}\right) = 35.51574-11.11193\times3.1 = 1.068757 \]</span></p>
<p>Of course, interpreting the log(odds) is difficult; thus, this value can be back-transformed to the odds by raising to the power of e.</p>
<p><span class="math display">\[ e^{\text{log}\left(\frac{\text{p}}{1-\text{p}}\right)} = e^{1.068757} \]</span> <span class="math display">\[ \frac{\text{p}}{1-\text{p}} = 2.911758 \]</span></p>
<p>Thus, a hoary bat with a 3.1 mm tooth height is 2.91 times more likely to be a <em>semotus</em> than a <em>cinereus</em> subspecies. Look at the fitplot above to convince yourself that this calculation makes sense.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<p>Of course, the researchers really would like to predict the probability that a bat is a <em>semotus</em>. Algebra on the best-fit line equation will result in an equation for making this prediction. Steps for the algebra are below</p>
<ul>
<li>Reminder of the equation of the best-fit line.</li>
</ul>
<p><span class="math display">\[ \text{log}\left(\frac{\text{p}}{1-\text{p}}\right) = \alpha+\beta X \]</span></p>
<ul>
<li>Both side of the equation set to the power of e (i.e., back-transforming the log). Note that this is an equation for predicting the odds as was demonstrated above; thus, substitute odds for <span class="math inline">\(e^{\alpha+\beta X}\)</span>.</li>
</ul>
<p><span class="math display">\[ e^{\text{log}\left(\frac{\text{p}}{1-\text{p}}\right)} = e^{\alpha+\beta X} \]</span> <span class="math display">\[ \frac{\text{p}}{1-\text{p}} = e^{\alpha+\beta X} \]</span> <span class="math display">\[ \frac{\text{p}}{1-\text{p}} = \text{odds} \]</span></p>
<ul>
<li>Multiply both sides by 1-p (which removes 1-p from the right-hand-side).</li>
</ul>
<p><span class="math display">\[ \text{p} = \text{odds}(1-\text{p}) \]</span></p>
<ul>
<li>Distribute the odds through the 1-p.</li>
</ul>
<p><span class="math display">\[ \text{p} = \text{odds}-\text{p}\text{odds} \]</span></p>
<ul>
<li>Add <span class="math inline">\(\text{p}\text{odds}\)</span> to both sides (which removes <span class="math inline">\(\text{p}\text{odds}\)</span> from the left-hand-side).</li>
</ul>
<p><span class="math display">\[ \text{p}+\text{p}\text{odds} = \text{odds} \]</span></p>
<ul>
<li>Factor out the p.</li>
</ul>
<p><span class="math display">\[ \text{p}(1+\text{odds}) = \text{odds} \]</span></p>
<ul>
<li>Divide both sides by <span class="math inline">\(1+\text{odds}\)</span> (which removes <span class="math inline">\(1+\text{odds}\)</span> from the right-hand-side).</li>
</ul>
<p><span class="math display">\[ \text{p} = \frac{\text{odds}}{1+\text{odds}} \]</span></p>
<p>Thus, the probability of “success” can be obtained as the ratio of the odds of “success” to 1 plus the odds of “success”. From above the odds that a hoary bat with a tooth height of 3.1 mm was the <em>semotus</em> subspecies was 2.911758. Using this last equation the probability that a bat with a tooth height of 3.1 mm is a <em>semotus</em> is <span class="math inline">\(\frac{2.911758}{1+2.911758}\)</span>=0.744361. Again, check the fitplot above to make sure that this value makes sense.</p>
<p>These predictions can be made in R with <code>predict()</code> very similarly to what you have done before. For example, the log odds computed by hand above may be computed with</p>
<pre class="r"><code>&gt; predict(glm1,data.frame(canine=3.1))</code></pre>
<pre><code>       1 
1.068746 </code></pre>
<p>The probability may be computed by including <code>type="response"</code> into <code>predict()</code>.</p>
<pre class="r"><code>&gt; predict(glm1,data.frame(canine=3.1),type=&quot;response&quot;)</code></pre>
<pre><code>        1 
0.7443584 </code></pre>
<p>The odds cannot be computed directly with <code>predict()</code>. If you want to see the odds you must either back-transform from the log(odds)</p>
<pre class="r"><code>&gt; exp(predict(glm1,data.frame(canine=3.1)))</code></pre>
<pre><code>       1 
2.911727 </code></pre>
<p>or compute the odds from the predicted probability</p>
<pre class="r"><code>&gt; p &lt;- predict(glm1,data.frame(canine=3.1),type=&quot;response&quot;)
&gt; p/(1-p)</code></pre>
<pre><code>       1 
2.911727 </code></pre>
<p><br></p>
</div>
<div id="predicting-x-with-certain-probability" class="section level2">
<h2>Predicting X with Certain Probability</h2>
<p>Researchers will also commonly use logistic regression results to predict the value of the quantitive explanatory varialbe (X) that would have a certain probability of “success.” For example, researchers may ask what the tooth height is such that there is an even probability that the bat would be <em>semotus</em> or <em>cinereus</em> (i.e., the probability of being <em>semotus</em> is 0.5) or the tooth height where the probability of being a <em>semotus</em> is 0.9. Visually picture choosing a probabiilty on the y-axis, morving horizontally until you hit the best-fit line, and then moving vertically to find the corresponding point on the x-axis.</p>
<p><img src="Lecture_LogReg_BatMorph_files/figure-html/unnamed-chunk-21-1.png" width="480" /></p>
<p>Of course, we want to be exact with this prediction. Again, we can perform some algebra on the equation of the line to solve for X.</p>
<ul>
<li>Reminder of the equation of the best-fit line.</li>
</ul>
<p><span class="math display">\[ \text{log}\left(\frac{\text{p}}{1-\text{p}}\right) = \alpha+\beta X \]</span></p>
<ul>
<li>Subtrace α from both sides (it will disappear from the right-hand side).</li>
</ul>
<p><span class="math display">\[ \text{log}\left(\frac{\text{p}}{1-\text{p}}\right) - \alpha = \beta X \]</span></p>
<ul>
<li>Divide both sides by β (it will disappear from the right-hand side)</li>
</ul>
<p><span class="math display">\[ \frac{\text{log}\left(\frac{\text{p}}{1-\text{p}}\right) - \alpha}{\beta} = X \]</span></p>
<ul>
<li>Simply flip the sides of the equals so that it looks like an equation for X.</li>
</ul>
<p><span class="math display">\[ X = \frac{\text{log}\left(\frac{\text{p}}{1-\text{p}}\right) - \alpha}{\beta} \]</span></p>
<p>Thus, the predicted tooth height for a probability of 0.5 is 3.196 as computed with</p>
<p><span class="math display">\[ X = \frac{\text{log}\left(\frac{0.5}{1-0.5}\right) - 35.51574}{-11.11193} \]</span></p>
<p>Make sure that this makes sense to you from the plot above.</p>
<p>Similarly, the predicted tooth height for a probability of 0.9 is 2.998 as computed with</p>
<p><span class="math display">\[ X = \frac{\text{log}\left(\frac{0.9}{1-0.9}\right) - 35.51574}{-11.11193} \]</span></p>
<p>Again, make sure that this makes sense to you from the plot above.</p>
</div>
<div id="confidence-intervals-for-predictions" class="section level2">
<h2>Confidence Intervals for Predictions</h2>
<div id="bootstrapping" class="section level3">
<h3>Bootstrapping</h3>
<p>As with all predictions, you want to use a confidence interval. Unfortunately, the predictions within a linear regression do not follow the normal distribution theory used for other linear models because the response variable is categorical (among other reasons). Confidence intervals can be constructed with a method called <em>bootstrapping.</em></p>
<p>Bootstrapping is a process that develops an approximate sampling distribution of a statistic by repeatedly sampling from the original data. Specifically, one bootstrap step randomly selects n individuals from the original data with replacement and computes the desired statistic. It then repeats this process many times (usually on the order of 5000-10000 times). All of the statistics from these repeated samples are then ordered from smallest to largest. The values of the statistics at the 2.5 and 97.5 percentiles are then found to form a 95% confidence interval for the statistic.</p>
<p>The bootstrapped samples can be generated with <code>bootCase()</code> which requires the object saved from <code>glm()</code> as its only argument. By default it will take 999 samples, which will be adequate for this class.</p>
<pre class="r"><code>&gt; bc1 &lt;- bootCase(glm1)      # bootstrapping, be patient!</code></pre>
<pre><code>&#39;bootCase&#39; is provided here only for backward compatibility.
Consider using &#39;Boot&#39; from the &#39;car&#39; package instead.</code></pre>
<p>As partially seen below the function returns parameter estimates for each of 999 bootstrapped samples (each row is a separate bootstrapped sample).</p>
<pre><code>     (Intercept)    canine
[1,]    41.60532 -13.04390
[2,]    34.83668 -10.86136
[3,]    47.59170 -14.96047
[4,]    42.05638 -13.11217
[5,]    37.61262 -11.68511</code></pre>
<p>A histogram of the slopes from the 999 bootstrapped samples is shown below. In addition, the vertical red lines show the values that have 2.5% and 97.% of the samples smaller and, thus, show the endpoints of a 95% bootstrapped confidence interval. Thus, one would be 95% confident that the slope for this logistic regression is between -16.27 and -8.37.</p>
<p><img src="Lecture_LogReg_BatMorph_files/figure-html/unnamed-chunk-24-1.png" width="336" /></p>
</div>
<div id="cis-for-predicted-probabilities" class="section level3">
<h3>CIs for Predicted Probabilities</h3>
<p>Bootstrapping is more useful to us when it comes to making confidence intervals for the predictions discussed above. First, however, we have to write a function that can be used to make the predictions for each of the bootstrapped samples. For example, the following function can be used to predict the probability of a “success” given a particular value of X.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>
<pre class="r"><code>&gt; predProb &lt;- function(x,alpha,beta) exp(alpha+beta*x)/(1+exp(alpha+beta*x))</code></pre>
<p>For example, this function can be used to find the probability of a <em>semotus</em> given a tooth height of 3.1 (and the obsered values of the intercept and slope).</p>
<pre class="r"><code>&gt; predProb(3.1,alpha=35.51574,beta=-11.11193)</code></pre>
<pre><code>[1] 0.7443605</code></pre>
<p>More importantly, the <code>alpha=</code> and <code>beta=</code> arguments can be the intercept and slope columns from the bootstrapped samples object. This then would predicted the probability of <em>semotus</em> if the tooth height is 3.1 for each bootstrapped sample (the first five are shown below).</p>
<pre class="r"><code>&gt; p31 &lt;- predProb(3.1,bc1[,1],bc1[,2])
&gt; p31[1:5]</code></pre>
<pre><code>[1] 0.7630067 0.7625069 0.7710459 0.8035546 0.8003989</code></pre>
<p>The <code>quantile()</code> function is used to identify the values in the 2.5% and 97.5% positions.</p>
<pre class="r"><code>&gt; quantile(p31,c(0.025,0.975))</code></pre>
<pre><code>     2.5%     97.5% 
0.6368349 0.8582176 </code></pre>
<p>Thus, one is 95% confident that the probability of being a <em>semotus</em> for a hoary bat with a 3.1 mm tooth height is between 0.64 and 0.86.</p>
</div>
<div id="cis-for-predicted-values-of-x-for-a-given-probability" class="section level3">
<h3>CIs for Predicted Values of X for a Given Probability</h3>
<p>The same process can be followed for making a confidence interval for the value of the quantitative explanatory variable for a certain probability. First, make a function to compute the value of X for a given probability.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
<pre class="r"><code>&gt; predX &lt;- function(p,alpha,beta) (log(p/(1-p))-alpha)/beta</code></pre>
<p>This is then applied to the boostrapped samples.</p>
<pre class="r"><code>&gt; x05 &lt;- predX(0.5,bc1[,1],bc1[,2])
&gt; quantile(x05,c(0.025,0.975))</code></pre>
<pre><code>    2.5%    97.5% 
3.153957 3.242064 </code></pre>
<p>Thus, one is 95% confident that the tooth height where it is an equal probability that the hoary bat is a <em>semotus</em> or a <em>cinereus</em> is between 3.15 and 3.24.</p>
<hr />
</div>
</div>
<div id="footnotes" class="section level2">
<h2>Footnotes</h2>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The researchers are trying to predict subspecies so it is the response variable. Thus, tooth height is the explanatory variable.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>The subspeces response variable is categorical (and binomial) and the tooth height explanatory variable is quantitative. Thus, this question requires a (binary) logistic regression.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>The range of tooth heights was less than 1cm. Thus, when interpreting the slope a “1cm increase in tooth height” was not realistic. Thus, this variable was multiplied by 10 to convert the cm to mm such that a slope would be for a “1mm increase in tooth height” and would thus would not be a larger increase then the range of the data.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>There are several arguments used in this <code>hist()</code> that you may not have seen before. The <code>w=</code> controls how wide the bins are, <code>ymax=</code> sets a common maximum value for the two y-axes, <code>ncol=</code> sets how many columns the plots will be placed in, and <code>nrow=</code> sets how many rows the plots will be placed in.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>As this does follow an exponential function then, from our work in the SLR module, only the response variable should be log-transformed. Thus, the log of the odds should be computed.<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>The “g” in <code>glm()</code> stands for “general.” If the <code>family=</code> argument is not used then <code>glm()</code> behaves exactly like <code>lm()</code>; i.e., all other models we have fit in this class could have also been fit with <code>glm()</code>. However, <code>glm()</code> is more general in the sense that it allows for residuals that are not normally distributed, which is the case with logistic regression.<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>The log(odds) are predicted by plugging the tooth height values into the best-fit line produced by <code>glm()</code> with coefficients shown by <code>coef()</code>. Remember that the best-fit line predicts log(odds).<a href="#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>When I look at the fitplot it appears to me that the probability that the bat is a <em>semotus</em> is around 0.7 or 0.75. If the probability was 0.75 then the odds would be <span class="math inline">\(\frac{0.75}{0.25}\)</span>=3, which is pretty close to the calculated 2.91 value.<a href="#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>This function is simply an R version of <span class="math inline">\(\text{p} = \frac{odds}{1+odds} = \frac{e^{\alpha+\beta X}}{1+e^{\alpha+\beta X}}\)</span>.<a href="#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>This function is simply and R version of <span class="math inline">\(X = \frac{\text{log}\left(\frac{\text{p}}{1-\text{p}}\right) - \alpha}{\beta}\)</span>.<a href="#fnref10" class="footnote-back">↩</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
