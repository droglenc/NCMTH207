---
title: "Model Concepts"
description: |
  Introduction to concepts of using and assessing the fit of linear models to data.
output:
  distill::distill_article:
    toc: true
    toc_depth: 3
    css: "../../css/distill_readings.css"
---

```{r setup, include=FALSE}
library(NCStats)
library(ggplot2)
library(patchwork)
knitr::opts_chunk$set(echo=FALSE,message=FALSE,comment="#R>  ",
                      fig.align='center',fig.width=3.5,fig.height=3.5)
clrs <- c("#1dabe6","#1c366a","#c3ced0","#e43034","#fc4e51","#af060f")[c(1,6)]
set.seed(34353)
## Aquaculture data
aqua <- read.csv("https://raw.githubusercontent.com/droglenc/NCData/master/BOD.csv") %>%
  mutate(srcjit=round(jitter(as.numeric(factor(src)),0.25),4))
aqua_mns <- group_by(aqua,src) %>% summarize(mn=mean(BOD))
aqua <- left_join(aqua,aqua_mns,by="src") %>%
  mutate(gmn=mean(BOD),
         residF=BOD-mn,
         residS=BOD-gmn)

## Mirex data
data(Mirex)
mirex_lm <- lm(mirex~weight,data=Mirex)

Mirex <- Mirex %>%
  mutate(pred=predict(mirex_lm),
         gmn=mean(mirex),
         residF=mirex-pred,
         residS=mirex-gmn)
```


# What is a Model
A model is a representation of something or some phenomena. It is usually a simplification or an abstraction that helps our understanding of the more complex reality. A mathematical or statistical model is an equation or system of equations that is meant to characterize the general characteristics of observations. Statistical models do not represent every observation perfectly, rather they attempt to best represent the "central tendency" of the observations. Weather forecasts are based on mathematical and statistical models. You have observed at least two statistical models in your introductory statistics course -- the mean and the regression line (Figure \@ref(fig:TwoModelsEx)).

<aside>
![Decoration](../zimgs/weather.jpg)
</aside>

```{r TwoModelsEx, echo=FALSE, fig.width=7, fig.cap="Two examples of models seen in your introductory statistics course -- two means (Left) and regression line (Right)."}
t2 <- ggplot(data=aqua) +
  geom_crossbar(mapping=aes(x=src,y=mn,ymin=mn,ymax=mn),width=0.25) +
  geom_point(mapping=aes(x=srcjit,y=BOD),alpha=0.25) +
  labs(y="Biological Oxygen Demand",x="Water Sample Location") +
  theme_NCStats()

slr <- ggplot(data=Mirex,mapping=aes(x=weight,y=mirex)) +
  geom_point(alpha=0.25) +
  geom_smooth(method="lm",se=FALSE,color="black") +
  labs(y="Mirex Concentration (mg/kg)",x="Weight (kg)") +
  theme_NCStats()

t2 + slr
```

&nbsp;

Models can predict an observation but generally not perfectly. For example, weather forecasters predict the temperature for tomorrow but will most likely be off by (hopefully only) a small amount.

An observed value of the response variable can be thought of as being equal to a value predicted from a model plus some deviation, or error, from that prediction; i.e.,

$$ \text{Observed Response} = \text{Model Predicted Response} + \text{error} $$

For example, tomorrow's temperature may be 74<sup>o</sup>F, which is the predicted 76<sup>o</sup>F from the forecaster's model plus -2<sup>o</sup>F "error."

In statistics, one model for predicting the response variable for an individual in a group is to use the mean for the group. My best guess at the height of an unknown student is to guess that they are average for "their group." Obviously, most individuals are not truly average, so the specific individual will deviate from the mean. In Figure \@ref(fig:FullResid1) an observation is shown as a red point, the predicted value for that individual is shown as a horizontal line at the mean for the individual's group, and the "error" from this prediction is shown as the vertical red line.

<aside>
We always predict the **response** variable with a model.
</aside>

```{r FullResid1, echo=FALSE, fig.cap='Biological oxygen demand versus sample location (points) with group means shown by horizontal segments. The residual from a model that uses a separate mean for both groups is shown.'}
t2 +
  geom_point(data=aqua[7,],mapping=aes(x=srcjit,y=BOD),color="red") +
  geom_segment(data=aqua[7,],mapping=aes(x=srcjit,xend=srcjit,y=BOD,yend=mn),
               color="red",linetype="dashed")
```

In the context of a simple linear regression, the predicted value is obtained by plugging the observed value of the explanatory variable into the regression equation. Thus, the "error" is the vertical distance between an observed point and the corresponding point on the line (Figure \@ref(fig:FullResid2)).

```{r FullResid2, echo=FALSE,message=FALSE, fig.cap='Mirex concentration versus fish weight with a simple linear regression line show. The residual from the regression line model is shown.'}
slr +
  geom_point(data=Mirex[7,],mapping=aes(x=weight,y=mirex),color="red") +
  geom_segment(data=Mirex[7,],mapping=aes(x=weight,xend=weight,y=mirex,yend=pred),
               color="red",linetype="dashed")
```

&nbsp;

Many hypothesis tests, including the two-sample t-test, can be cast in a framework of competing statistical models. Using this framework requires assessing the relative fit (to data) and complexity of a model. The remainder of this module is about measuring fit and complexity of models. We will discuss fit and formally compare two models to see which is "best" in the next module.

&nbsp;

# Assessing Fit (SS)
### A Residual
A residual is an estimate of the "error" discussed in the previous section. If you rearrange the formula shown above and replace "error" with "residual" you see that

$$ \text{residual} = \text{Observed Response} - \text{Model Predicted Response} $$

Visually a residual is the vertical distance between a point and the "model", as shown by the vertical dashed lines above (or further below). Residuals are vertical distances because they are the difference between two values of the response variable, which is always plotted on the y-axis.

<aside>
Residuals are *vertical* distances between an observation and the model.
</aside>

Residuals are negative if the point is "below" the model prediction and positive if the point is "above" the model prediction. More importantly, the absolute value of the residual is a measure of how close the model prediction is to the point or how well the model fits the individual point. Large residuals (in an absolute value sense) mean that the point is far from the model prediction and, thus, the model does not represent that point very well. Points with small residuals, in contrast, are near the model prediction and are thus well-represented by the model. Figure \@ref(fig:FullResid3) shows points with relatively large residuals in red and relatively small residuals in blue.

```{r FullResid3, echo=FALSE, fig.width=7, fig.cap='Same plots as Figure \\@ref(fig:FullResid1) and Figure \\@ref(fig:FullResid2) but with a "large" residual shown in red and a "small" residual shown in blue.'}
t2a <- t2 +
  geom_point(data=aqua[20,],mapping=aes(x=srcjit,y=BOD),color="red") +
  geom_segment(data=aqua[20,],mapping=aes(x=srcjit,xend=srcjit,y=BOD,yend=mn),
               color="red",linetype="dashed") +
  geom_point(data=aqua[9,],mapping=aes(x=srcjit,y=BOD),color="blue") +
  geom_segment(data=aqua[9,],mapping=aes(x=srcjit,xend=srcjit,y=BOD,yend=mn),
               color="blue",linetype="dashed")
slra <- slr +
  geom_point(data=Mirex[7,],mapping=aes(x=weight,y=mirex),color="red") +
  geom_segment(data=Mirex[7,],mapping=aes(x=weight,xend=weight,y=mirex,yend=pred),
               color="red",linetype="dashed") +
  geom_point(data=Mirex[78,],mapping=aes(x=weight,y=mirex),color="blue") +
  geom_segment(data=Mirex[78,],mapping=aes(x=weight,xend=weight,y=mirex,yend=pred),
               color="blue",linetype="dashed")
t2a + slra
```

&nbsp;

### Residual Sum-of-Squares
If a residual measures how closely a model comes to a point then it stands to reason that the sum of all of the residuals measures how closely a model comes to all of the points. Unfortunately, because residuals are negative and positive they always sum to 0.^[Under certain reasonable assumptions.] Thus, the sum of all residuals is not a useful measure of the overall fit of a model.

Instead of summing residuals, statisticians sum squared residuals into a quantity called a **residual sum-of-squares (RSS)**.^[Some statisticans call this an error sum-of-squares or a sum of squared errors (SSE)] Using the formula for a residual from above, an RSS for a given set of observed data and a model is computed with

<aside>
The RSS measures how closely the model comes to *all* of the observations.
</aside>

$$ \text{RSS} = \sum_{data}\left(\text{Observed Response}-\text{Model Predicted Response}\right)^2 $$

The RSS is on an unfamiliar scale (squared residuals?) but it maintains the same conceptual idea that summing residuals would have. Mainly, the smaller the RSS the more closely the points are to the model. The full set of residuals required to compute an RSS are shown in Figure \@ref(fig:RSS1).

```{r RSS1, echo=FALSE, fig.width=7, fig.cap='Same plots as Figure \\@ref(fig:FullResid1) and Figure \\@ref(fig:FullResid2) but with all residuals shown.'}
t2b <- t2 +
  geom_segment(data=aqua,mapping=aes(x=srcjit,xend=srcjit,y=BOD,yend=mn),
               color="red",linetype="dashed")
slrb <- slr +
  geom_segment(data=Mirex,mapping=aes(x=weight,xend=weight,y=mirex,yend=pred),
               color="red",linetype="dashed")
t2b + slrb
```

&nbsp;

As a value, the RSS is a measure of how *poorly* the model fits the data -- i.e., small values are a good fit, large values are a poor fit. Thus, the RSS is often called "a measure of lack-of-fit" of the model to the observations.

<aside>
An RSS is a measure of the "lack-of-fit" of a model to the data.
</aside>

Unfortunately, the magnitude of the RSS is only useful in comparison to other RSS computed for different models from the same data. We will discuss this further in the next module.

&nbsp;

# Residual Degrees-of-Freedom
You used degrees-of-freedom (df) with t-tests and chi-square tests in your introductory statistics course. However, you likely did not discuss what degrees-of-freedom mean and where they come from. I will discuss this briefly here, but we will use df more in the next module.

Residual degrees-of-freedom (Rdf) are the number of observations that are "free" to vary if the sample size ($n$) and number of parameters estimated is known. As a simple example, suppose that we know that $\bar{x}$=13 from $n$=4 observations. With just this information can I tell you the values for the four observations that went into $\bar{x}$? Clearly I cannot. If you give me one observation can I tell you the remaining three? No! If you tell me two? No! If you tell me three observations can I tell you the last observation? Yes, because the total of the four numbers must be 52 (=$n\bar{x}$=4&times;13); so the last number must be 52 minus the total of the three numbers you told me. In this case, three numbers were "free" to be any value before the last number was set. Thus, this case has three residual degrees-of-freedom.

Residual degrees-of-freedom are more complicated to explain in other situations, but generally

$$ \text{Rdf}=\text{Number of Observations}-\text{Number of Model Parameters} $$

In the example above, there were four observations (n) and one model parameter -- $\bar{x}$ -- so df=4-1=3. In Figure \@ref(fig:TwoModelsEx)-Left there are `r nrow(aqua)` observations and two parameters (i.e., two group means) so Rdf=`r nrow(aqua)`-2=`r nrow(aqua)-2`. In Figure \@ref(fig:TwoModelsEx)-Right there are `r nrow(Mirex)` observations and two parameters (i.e., the slope and intercept of the regression line) so Rdf=`r nrow(Mirex)`-2=`r nrow(Mirex)-2`.

As a general rule, parameter estimates are more precisely estimated with more residual degrees-of-freedom. Thus, models that "preserve" residual degrees-of-freedom (i.e., have fewer parameters) are preferred, all else being equal.

&nbsp;

# Mean-Squares
Sums-of-squares are useful measures of model fit, but they are largely uninterpretible on their own. However, if a sum-of-squares is divided by its corresponding degrees-of-freedom it is called a **Mean-Square (MS)**. Mean-squares are the **variance** (i.e., squared standard deviation) of individuals around a given model. Mean-squares have useful mathematical properties as you will see in future modules. However, visually the square root of a mean square loosely describes how far each point is from the model (i.e., the "errors"), on average. The mean-squares are thus a measure of the "noise" around each model.

<aside>
MS are variances; thus, the square root of MS are standard deviations
</aside>

# Corrections {.appendix}
If you see any errors (code, typographical, or logical) please bring these to [my attention](mailto:dogle@northland.edu).
