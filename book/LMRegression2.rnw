<<echo=FALSE, cache=FALSE, results='hide'>>=
set_parent('Biometry.Rnw')
@

\chapter{One-Way IVR}  \label{chap:LMRegression2}
  \vspace{0pt}
    \begin{ChapObj}{\boxwidth}
      \textbf{Chapter Objectives:}
        \begin{Enumerate}
          \item Understand how indicator variable are created and what they are used for.
          \item Understand how interaction variables are created and what they are used for.
          \item Describe the order that indicator and interaction variables are entered into models.
          \item Understand how to perform hypothesis tests of equal y-intercepts or slopes among groups.
          \item Understand how to interpret the results of an ANCOVA.
          \item Describe the benefits of an ANCOVA versus the creation of ratios or the use of residuals.
        \end{Enumerate}
    \end{ChapObj}

\minitoc
\newpage

\lettrine{S}{imple linear regression is a powerful} research tool used to evaluate the relationship between two quantitative variables \chaprefp{chap:LMRegression1}.  It is a common occurrence for a researcher to need to compare simple linear regression models fit to separate groups of individuals.  A method, which will be called \emph{indicator variable regression}\footnote{This is not a standard name.  Some authors call it \emph{dummy variable regression} (e.g., \cite{Fox1997}), whereas others call it \emph{analysis of covariance} (ANCOVA).  I will not use either of these terms as I prefer the word ``indicator'' to ``dummy'' and I will reserve the ANCOVA method for the situation where the separate SLR models are known or assumed to have equal slopes.  Thus, ANCOVA models are a subset of the IVR models discussed here.  It should also be noted that some authors (e.g., \cite{Weisberg2005}) do not use a separate name for IVR but simply include it as a multiple linear regression model.} (IVR), is used to make these comparisons.  For example, IVR methods could be used in each of the following situations:
\begin{Enumerate}
  \item Determine if the relationship between foot length and tail length of possums differs between possums collected in Victoria and possums collected elsewhere \citep{WoodsHellgren2003}.
  \item Determine if the relationship between propodus height and claw closing force differs among three types of crabs \citep{YamadaBoulding1998}.
  \item Determine if the proportional area covered by an invasive plant differs between sites after adjusting for different resident times for the plant (i.e., how long it has been present at a site) \citep{Mullerova2005}.
  \item Determine if the snout-to-vent length (SVL) to body mass relationship differs between sexes of iguanid lizards \citep{VittGoldberg1983}.
  \item Determine if the liver weight of yellow perch, adjusted for somatic weight, differed between four locations -- two reference sites and two sites potentially impacted by pulp and paper mills \citep{KarelsOikari2000}.
  \item Determine if the relationship between total body electrical conductivity and lean body mass (a method used to measure body fat) differs between live and dead birds of various species \citep{Castroetal1990}.
  \item Determine if the relationship between the body mass of porcupines (\emph{Erithizon dorsatum}) and days since October 15th differs among life stages AND sexes \citep{SweitzerBerger1993}.
  \item Determine if the relationship between clutch size and female length for spiders differs depending on whether the spider species is from web-building or cursorial genera \citep{Simpson1995}.
\end{Enumerate}

In each of these cases, a single quantitative explanatory variable is considered along with one factor explanatory variable.  Because there is only one factor variable, these specific situations require a \emph{one-way IVR model}.  Furthermore, it is common practice to call the quantitative explanatory variable in these models a \emph{covariate}.  This terminology will be used throughout the rest of this chapter.

\defn{One-way IVR}{A regression model derived from a single quantitative explanatory variable and a single factor variable.}

\vspace{-12pt}
\defn{Covariate}{The quantitative explanatory variable in an IVR model.}


\section{Indicator Variables} \label{sect:IndicatorVars}
A \emph{factor} variable\footnote{Factor variables were considered extensively in the ANOVA chapters (see \chapref{chap:LMANOVA1} and \chapref{chap:LMANOVA2}).} is a categorical variable with two or more levels \citep{Weisberg2005}.  A factor variable must be converted to  numeric codes in order to be included in a linear regression model.  An indicator variable is the variable that contains the numeric ``codes'' created from the factor variable.  Specifically, an \emph{indicator variable} is a variable that consists of zeroes and ones\footnote{Other coding schemes exist and can be used.  For example, another popular coding scheme, called ``contrast coding'', uses -1 and 1.  However, the 0 and 1 coding scheme leads to simpler interpretations of coefficients and calculations.  Thus, the 0 and 1 coding scheme will be used throughout these notes.}, where a one indicates that the individual has a certain characteristic and a zero indicates that it does not have the characteristic.

\defn{Indicator Variable}{A variable that is a numerical representation of a dichotomous (or binary or binomial) categorical variable.}

\vspace{-12pt}
\warn{Indicator variables allow the inclusion of categorical variables into regression analyses.}

\vspace{-12pt}
\warn{Indicator variables will be coded as 0 for individuals that do NOT have the characteristic of interest and 1 for individuals that do have the characteristic of interest.}

An indicator variable should be named after the characteristic denoted by the ones so that it will be easy to remember what the indicator variable represents and how the coding was made\footnote{Indicator variables will be named automatically by R.  Fortunately, it follows the naming rule suggested here.}.  For example, if a variable is called \var{CONTROL} then a 0 means the individual was not in the control group and 1 means that it was in the control group.  If a variable is called \var{FEMALES} then a 0 means that the individual is not a female (i.e., a male) and a 1 means that the individual is a female.

\warn{Indicator variables should be named after the ``1'' group so that it is easy to remember the coding scheme being used.}

As an example, \cite{Makarewicz2003} studied the relationship between the concentration of mirex\footnote{Mirex is a chlorinated hydrocarbon that was commercialized as an insecticide.  Use of mirex was banned in 1976 because of its impact on the environment.} in the tissues of salmon and the weight of the salmon.  They collected these data on two species of salmon -- coho (\emph{Oncorhynchus kisutch}) and chinook salmon (\emph{Oncorhynchus tshawytscha}) -- and were interested in determining if the relationship between mirex and weight differed between these two species.  For this purpose, an indicator variable could be defined from the \var{species} factor variable as such\footnote{This indicator variable could have been named \var{coho} for succinctness.  However, the name chosen here will be consistent with names for indicator variables automatically created by R.  In addition, the indicator variable could have been chosen to represent chinook salmon.},

\[
  speciescoho = \left\{\begin{array}{l}
  1 \text{ if a coho salmon }\\
  0 \text{ if NOT a coho salmon}
  \end{array}  \right.
\]

Combinations of indicator variables can be used to code factor variables that consist of more than two levels.  For example, \cite{Makarewicz2003} also were interested in determining if the relationship between mirex concentrations and weight of the salmon differed among six different years of collection.  For simplicity and space considerations, let's initially consider that they were only interested in three years -- 1977, 1982, and 1986.  In this simpler situation, two indicator variables would be derived from the \var{year} factor variable such as,

\[
  year1982 = \left\{\begin{array}{l}
  1 \text{ if collected in 1982 }\\
  0 \text{ if NOT collected in 1982 }
  \end{array}  \right.
\]
and
\[
  year1986 = \left\{\begin{array}{l}
  1 \text{ if collected in 1986 }\\
  0 \text{ if NOT collected in 1986}
  \end{array}  \right.
\]

Note that it is possible to create a third dummy variable to represent salmon collected in 1977.  However, this indicator variable would be redundant with \var{year1982} and \var{year1986}.  In other words, if it is known that \var{year1982}=0 and that \var{year1986}=0 then the salmon must have been collected in 1977.  This example illustrates the rule that the number of indicator variables required is one less than the number of levels to be coded\footnote{This is only true if an intercept term is contained in the regression model.  All models in these notes will contain the intercept term.}.  Thus, five indicator variables would be needed to include information from all six years that salmon were collected by \cite{Makarewicz2003}.

\warn{One less indicator variable is needed then the number of categories in the variable.}

The level denoted by the group with ``0''s for all related indicator variables is called the \emph{reference group}.  In the last example above, the salmon collected in 1977 would be considered the reference group.  It will be shown in a later section that all comparisons of intercepts and slopes in the analysis will ``refer'' to this group of individuals.

\defn{Reference group}{The category or group that is represented by zeroes for all indicator variables.}

Suppose, for illustration purposes, that fish collected in 1982 were to be considered the reference group rather than fish collected in 1977.  In this situation, the \var{year1982} indicator variable would be replaced with the \var{year1977} indicator variable in the model.

\warn{Changing the reference group in an analysis requires changing the indicator variables used in the analysis.}


\section{Interaction Variables}
An \emph{interaction variable} is an explanatory variable that is the product of two or more other explanatory variables (see \chapref{chap:LMANOVA2}).  In IVR models, the interaction between the covariate and the indicator variables are used to determine if the effect of the covariate on the response variable depends on the levels represented by the indicator variables.  In other words, these interaction variables allow one to model the influence of a factor variable on the covariate's effect on the response variable\footnote{This is the same general interpretation of interaction variables as discussed for a two-way ANOVA.}.

\defn{Interaction Variable}{An explanatory variable that is the product of two or more explanatory variables.}

\vspace{-12pt}
\warn{Interaction variables in IVR are used to determine if the effect of the covariate on the response variable differs among groups defined by the indicator variable(s).}

The interaction between two variables in an IVR model may best be illustrated by looking at the examples illustrated in \figref{fig:IVRInteraction}.  In the graph on the left, the effect of salmon weight (the covariate) on mirex concentration in the tissue (the response) is the same for coho and chinook salmon.  This is illustrated by each group having the same slope\footnote{In regression the ``effect'' of the explanatory variable on the response variable is measured by the slope.}.  However, in the graph on the right, different slopes indicate that the effect of salmon weight on mirex concentration in the tissue differs between the two species.  In this particular example, the relationship is ``flatter'' for the coho salmon than for the chinook salmon.  Thus, an interaction exists for the situation depicted in \figref{fig:IVRInteraction}-Right because the effect of the covariate on the response depends on which group is being considered.  Graphically, an interaction is illustrated by non-parallel lines.

<<IVRInteraction, echo=FALSE, fig.cap="Idealized fitted-line plots representing SLR fit to two groups.  The graph on the left indicates the absence of an interaction.  The graph on the right indicates the presence of an interaction.", fig.pos="!h">>=
 x <- c(0,1)
 y1p <- c(0,.8);   y2p <- c(0.2,1)  #parallel lines
 y1i <- c(0.2,.8); y2i <- c(0,1)    #interaction lines
 plot(x,y2i,col="white",xlab="Weight",ylab="Mirex Concentration",xaxt="n",yaxt="n")
 lines(x,y1p,lwd=2);  lines(x,y2p,lwd=2,col="red")
 legend("topleft",legend=c("Coho","Chinook"),col=c("black","red"),lwd=2,lty=1,cex=.8)
 plot(x,y2i,col="white",xlab="Weight",ylab="Mirex Concentration",xaxt="n",yaxt="n")
 lines(x,y1i,lwd=2);  lines(x,y2i,lwd=2,col="red")
@

\warn{Parallel lines indicate non-interaction.  Intersecting lines (actual or eventual) indicate an interaction.}


\section{Models \& Sub-Models}
Indicator and interaction variables can be used to determine if simple linear regression lines fit to different groups differ among those groups.  To illustrate this point, consider a situation with one quantitative response variable ($Y$), one covariate ($X$), and one indicator variable ($Z$) representing a single dichotomous factor variable\footnote{This situation is exactly the situation that exists for examining the effect of weight on mirex concentration between the two species of salmon.}.  In addition, an interaction between the covariate and the indicator variable is constructed.  Thus, the generic model fit for these data is

\begin{equation} \label{eqn:IVRModel1}
  \mu_{Y|X,Z} = \alpha+\beta_{1}X+\delta_{1}Z+\gamma_{1}X*Z
\end{equation}

The following symbols should be used for parameters, as was illustrated in this model, to make the interpretation of future models simpler

\begin{Itemize}
  \item $\alpha$: an overall intercept term (see below).
  \item $\beta_{1}$: coefficient on the covariate (overall slope; see below).
  \item $\delta_{i}$: coefficient on the ith indicator variable.
  \item $\gamma_{i}$: coefficient on the interaction between the covariate and the ith indicator variable.
\end{Itemize}

The model in \eqref{eqn:IVRModel1} can be reduced to so-called \emph{submodels} that correspond to each group.  The reduction of an overall regression model to appropriate submodels is accomplished by substituting appropriate values for the indicator variable into the overall regression model.  In other words, the submodel for the reference group represented in \eqref{eqn:IVRModel1} is found by substituting a \var{0} for \var{Z} in \eqref{eqn:IVRModel1} and then simplifying\footnote{All coefficients multiplied by a zero ``drop out'' of the model.}.  Similarly, the submodel for the non-reference group is found by substituting a \var{1} for \var{Z} in \eqref{eqn:IVRModel1} and simplifying\footnote{All coefficients multiplied by one can be simplified and combined with other coefficients}.  These substitutions and simplifications are shown below,

\begin{center}
  \begin{tabular}{||c|c|l||}
    \hline
      \widen{-3}{8}{\textbf{Group}}& \textbf{Z=} & \textbf{Submodel ($\mu_{Y|\cdots}=$)}\\
    \hline
      \widen{-2}{7}{Does NOT Have}& & $=\alpha+\beta_{1}X+\delta_{1}*0+\gamma_{1}*0*X$\\
      (\emph{Reference}) &\rb{1.5}{0}&\widen{-3}{5}{$=\alpha+\beta_{1}X$}\\
    \hline
      \widen{-2}{7}{}&&$=\alpha+\beta_{1}X+\delta_{1}*1+\gamma_{1}*1*X$\\
      \widen{-2}{5}{}&&$=\alpha+\beta_{1}X+\delta_{1}+\gamma_{1}X$\\
      \rb{3}{Does Have}&\rb{3}{1}&\widen{-3}{6}{$=(\alpha+\delta_{1})+(\beta_{1}+\gamma_{1})X$}\\
    \hline
  \end{tabular}
\end{center}

\warn{Submodels are found by appropriately substituting ``0''s and ``1''s for the indicator variable(s) in the overall IVR model.}

Examination of the simplified submodels for each group shows that each is, itself, a linear model.  The submodel for the ``Does NOT Have'' group indicates that this group is modeled with a slope of $\beta_{1}$ and an intercept of $\alpha$.  In contrast, the submodel for the ``Does Have'' group has a slope of $\beta_{1}+\gamma_{1}$ and an intercept of $\alpha+\delta_{1}$.  Furthermore, from

\[
  \text{``Does Have'' Intercept} - \text{``Does Not Have'' Intercept} = (\alpha+\delta_{1}) - \alpha = \delta_{1}
\]

it is seen that $\delta_{1}$ is the difference in intercept between the ``Does Have'' and the ``Does Not Have'' groups.  A similar computation can be made to show that $\gamma_{1}$ is the difference in slope between the ``Does Have'' and the ``Does Not Have'' groups.  Thus, the interpretations of the four coefficients from the fit of \eqref{eqn:IVRModel1} are summarized in \figref{fig:IVRSubmodels} and as follows,

\begin{Itemize}
  \item $\alpha$: intercept for the ``Does NOT Have'' group
  \item $\beta_{1}$: slope for the ``Does NOT Have'' group
  \item $\delta_{1}$: \emph{difference in intercept} between the ``Does Have'' and the ``Does NOT Have''
  \item $\gamma_{1}$: \emph{difference in slope} between the ``Does Have'' and the ``Does NOT Have''
\end{Itemize}

<<IVRSubmodels, echo=FALSE, fig.scap="Sub-model fits.", fig.scap="IVR fit and parameter meanings.", fig.cap="Representation of two sub-models fit with \\eqref{eqn:IVRModel1} and the meaning of the parameter estimates.  Note that the black line is the ``reference'' sub-model.", fig.pos="h">>=
 par(mar=c(2,2,1,1), mgp=c(0.5,0,0),las=1,tcl=-0.2)
 a <- 2; b1 <- 0.4; d1 <- -1.6; g1 <- 0.45
 max.x <- 4
 plot(c(0,max.x),c(a,a+max.x*b1),type="l",lwd=3,ylim=c(0,max.x),xlim=c(-1.55,max.x),xlab="X",ylab="Y",xaxt="n",yaxt="n",cex.lab=1.5)  # First submodel
 lines(c(0,max.x),c(a+d1,a+d1+(b1+g1)*max.x),type="l",lwd=3,col="blue")                                      # Second submodel
 points(0,a,pch=19,cex=1.3); text(0,a,expression(alpha),pos=2,cex=1.3)                                       # First intercept (alpha)
 points(0,a+d1,pch=19,col="blue",cex=1.3); text(0,a+d1,expression(alpha+delta[1]),col="blue",pos=2,cex=1.3)  # Second intercept (alpha + delta)
 lines(c(0,1,1),c(a,a,a+b1),lty=3,lwd=3); text(1,a+b1/2,expression(beta[1]),pos=4,cex=1.3)                   # First slope (beta)
 lines(c(0,1,1),c(a+d1,a+d1,a+d1+b1+g1),lty=3,lwd=3,col="blue")                                              # Second slope (beta + gamma)
 text(1,a+d1+(b1+g1)/2,expression(beta[1]+gamma[1]),col="blue",pos=4,cex=1.3)                                #     label beta+gamma
 lines(c(0,0),c(a,a+d1),lwd=3,lty=3,col="red"); text(0,a+d1/2,expression(delta[1]),col="red",pos=2,cex=1.3)  # delta distance
 x<- -d1/(b1+g1)                                                                                             # X coord for other second slope
 lines(c(x,x+1,x+1),c(a,a,a+b1),lty=3,lwd=3); text(x+1,a+b1/2,expression(beta[1]),pos=4,cex=1.3)             # beta portion of beta + gamma slope
 lines(c(x+1,x+1),c(a+b1,a+b1+g1),lty=3,lwd=3,col="red")                                                     # gamma portion of beta + gamma slope
 text(x+1,a+b1+g1/2,expression(gamma[1]),col="red",pos=4,cex=1.3)                                            #   label gamma
 text(0,-0.35,0,xpd=TRUE,cex=1.25)                                                                           # label x=0
@

These interpretations show why the ``Does NOT Have'' group is called the reference group.  The coefficients in the reference submodel represent the intercept and slope for that group.  These coefficients also appear in the other submodels.  The additional coefficients in the non-reference group submodels represent differences between the non-reference group and the reference group -- every difference is computed relative to the value for the reference group.  Thus, all other coefficients are only meaningful relative to the reference group.

\warn{The intercept coefficient and the coefficient on the quantitative explanatory variable are estimates of the intercept and slope, respectively, for the reference group.}

\vspace{-12pt}
\warn{All coefficients on indicator variables are estimated differences in \emph{intercepts} between a non-reference group and the reference group.}

\vspace{-12pt}
\warn{All coefficients on interaction variables are estimated differences in \emph{slopes} between a non-reference group and the reference group.}

Further note that, with the suggested parameter symbols, the $\delta$ coefficients will represent differences in intercepts and the $\gamma$ coefficients will represent differences in slopes.

\warn{$\delta$ coefficients represent the difference in \emph{intercept} between some non-reference group and the reference group.}

\vspace{-12pt}
\warn{$\gamma$ coefficients represent the difference in \emph{slope} between some non-reference group and the reference group.}

Thus, the very simple model in \eqref{eqn:IVRModel1} is used to determine which of four situations best represents the data \figrefp{fig:IVRIdealPlot1}:
\begin{Itemize}
  \item A single regression line represents the relationship between the response variable and the covariate for both groups (i.e., $\delta_{1}=0$ and $\gamma_{1}=0$),
  \item Separate parallel lines with different y-intercepts are needed for both groups (i.e., $\delta_{1}\neq0$ and $\gamma_{1}=0$),
  \item Separate non-parallel lines with the same y-intercept are needed for both groups (i.e., $\delta_{1}=0$ and $\gamma_{1}\neq0$),
  \item Two completely separate lines are needed for both groups (i.e., $\delta_{1}\neq0$ and $\gamma_{1}\neq0$).
\end{Itemize}

<<IVRIdealPlot1, echo=FALSE, fig.width=7, fig.height=7, out.width='.8\\linewidth', fig.cap="Hypothetical depictions of four situations that can occur for the relationship between a response variable ($Y$) and a covariate ($X$) for two groups of data.", fig.pos="h">>=
 par(mar=c(2.5,2.5,2,1), mgp=c(1,0.75,0),mfrow=c(2,2),las=1,tcl=-0.2)
 x <- c(0,1)
 y1 <- c(0,0.2); y2 <- c(0.8,1)
 plot(x,c(y1[1],y2[2]),xlab="X",ylab="Y",type="l",lwd=2,xlim=c(0,1),ylim=c(0,1),cex.lab=1.25,xaxt="n",yaxt="n")
 lines(x,c(y1[1],y2[2])-0.01,lwd=2,col="red")
 mtext("Coincident Lines",cex=1.15,line=0.5)
 plot(x,c(y1[1],y2[1]),xlab="X",ylab="Y",type="l",lwd=2,xlim=c(0,1),ylim=c(0,1),cex.lab=1.25,xaxt="n",yaxt="n")
 lines(x,c(y1[2],y2[2]),lwd=2,col="red")
 mtext("Parallel Lines",cex=1.15,line=0.5)
 plot(x,c(y1[2],y2[1]),xlab="X",ylab="Y",type="l",lwd=2,xlim=c(0,1),ylim=c(0,1),cex.lab=1.25,xaxt="n",yaxt="n")
 lines(x,c(y1[2],y2[2]),lwd=2,col="red")
 mtext("Diff Slope, Same Intercept",cex=1.15,line=0.5)
 plot(x,c(y1[2],y2[1]),xlab="X",ylab="Y",type="l",lwd=2,xlim=c(0,1),ylim=c(0,1),cex.lab=1.25,xaxt="n",yaxt="n")
 lines(x,c(y1[1],y2[2]),lwd=2,col="red")
 mtext("Diff Slope, Diff Intercept",cex=1.15,line=0.5)
@

The order for including the covariate, indicator, and interaction variables can be set in order to allow consistent and simple interpretations.  In general, it is best to enter all single covariates to the model first, followed by all indicator variables, and then the interaction variables.  This is the general rule illustrated with the models above and all subsequent models.

\warn{As a general rule, the variables are entered into an IVR in the following order: (1) all single quantitative explanatory variables, (2) all indicator variables, and (3) all interaction variables.}

As a specific example, the following model is used to determine if the relationship between mirex concentration (\var{mirex}) and weight (\var{weight}) differs between coho and chinook salmon,

\begin{equation} \label{eqn:IVRSalmonModel1}
  \mu_{mirex|weight,speciescoho} = \alpha+\beta_{1}weight+\delta_{1}speciescoho+\gamma_{1}speciescoho*weight
\end{equation}

The submodels corresponding to each group are found by substituting appropriate values for the indicator variable, \var{speciescoho}, into \eqref{eqn:IVRSalmonModel1}.  These substitutions and reductions are shown below,

\begin{center}
  \begin{tabular}{||c|c|l||}
    \hline
      \widen{-3}{8}{\textbf{Group}} & \var{\textbf{speciescoho}=} & \textbf{Submodel ($\mu_{mirex|\cdots}=$)}\\
    \hline
      \widen{-2}{7}{Chinook}& & $=\alpha+\beta_{1}weight+\delta_{1}*0+\gamma_{1}*0*weight$ \\
       (\emph{reference})& \rb{1.5}{0} & \widen{-3}{5}{$=\alpha+\beta_{1}weight$} \\
    \hline
      \widen{-2}{7}{}& &$=\alpha+\beta_{1}weight+\delta_{1}*1+\gamma_{1}*1*weight$ \\
      \widen{-2}{5}{}& &$=\alpha+\beta_{1}weight+\delta_{1}+\gamma_{1}weight$ \\
      \rb{3}{Coho}& \rb{1.5}{1} &\widen{-3}{6}{$=(\alpha+\delta_{1})+(\beta_{1}+\gamma_{1})weight$} \\
    \hline
  \end{tabular}
\end{center}

Thus, the interpretation of the coefficients fit with \eqref{eqn:IVRSalmonModel1} are,

\begin{Itemize}
  \item $\alpha$: intercept for chinook salmon
  \item $\beta_{1}$: slope for chinook salmon
  \item $\delta_{1}$: \emph{difference in intercept} between coho and chinook salmon
  \item $\gamma_{1}$: \emph{difference in slope} between coho and chinook salmon
\end{Itemize}

\subsubsection*{Fitting Models, Extracting Coefficients, \& Making Predictions in R}
Fitting the full model used to make comparisons among groups is exceptionally easy to do in R because R automatically creates the required indicator and interaction variables\footnote{Unfortunately, this ease comes at the cost of the actual model to be fit being somewhat transparent to the user.  The underlying structure of the model (e.g., indicator variables) can be viewed by sending the saved \R{lm} object to \R{model.matrix()}.}.  The full model can be fit to the data with a formula argument in \R{lm()} that is similar to what was used for a two-way ANOVA.  Specifically, the full model is fit with a formula of the form \R{response}$\sim$\R{covariate*factor} where \R{response} represents the quantitative response variable, \R{covariate} represents the quantitative explanatory variable, and \R{factor} represents the factor explanatory variable\footnote{As with two-way ANOVA, this simple declaration, using an asterisk to combine the covariate and the factor variable, is an efficient way to tell R to include both main variables and the interaction term.  In other words, the \R{response}$\sim$\R{covariate*factor} code is shorthand for the \R{response}$\sim$\R{covariate+factor+covariate:factor} code.}.  Estimates of the parameters identified above (i.e., $\alpha$, $\beta_{1}$, $\delta_{1}$, and $\gamma_{1}$) are obtained by submitting the saved \R{lm} object to \R{summary()}.

As a specific example, consider that the mirex concentration in salmon data consists of these variables,
\begin{Enumerate}
  \item \var{mirex}: the measured mirex concentration in the tissue ($mg\cdot kg^{-1}$).
  \item \var{weight}: the measured weight of the fish ($kg$).
  \item \var{species}: a factor variable indicating the species (\var{coho} or \var{chinook}).
\end{Enumerate}
These data are ``loaded'' and prepared for use with,
<<>>=
data(Mirex)
@

The model expressed in \eqref{eqn:IVRSalmonModel1} is then fit\footnote{The order of the variables should be as discussed previously -- covariate followed by the indicator variable.} and the coefficient summaries are obtained with

<<echo=-3>>=
mirex.lm1 <- lm(mirex~weight*species,data=Mirex)
summary(mirex.lm1)
sum.m1 <- summary(mirex.lm1)
@

The estimates for the parameters are found under the ``Estimate'' column.  Thus, $\hat{\alpha}$=\Sexpr{formatC(coef(mirex.lm1)[1],format="f",digits=4)}, $\hat{\beta}_{1}$=\Sexpr{formatC(coef(mirex.lm1)[2],format="f",digits=4)}, $\hat{\delta}_{1}$=\Sexpr{formatC(coef(mirex.lm1)[3],format="f",digits=4)}, and $\hat{\gamma}_{1}$=\Sexpr{formatC(coef(mirex.lm1)[4],format="f",digits=4)}.  Thus, for example, the sample slope for chinook salmon is \Sexpr{formatC(coef(mirex.lm1)[2],format="f",digits=4)} and the sample slope for coho salmon is \Sexpr{formatC(-1*coef(mirex.lm1)[4],format="f",digits=4)} smaller than the sample slope for chinook salmon.  Of course, inferential methods are needed to determine if this result suggests a real difference in the populations.  For example, a statistical test of whether $\gamma_{1}=0$ would be used to determine if the population slope differed between coho and chinook salmon.  These types of tests are the subject of a subsequent section.

Confidence intervals for the coefficients can be obtained by submitting the saved \R{lm} object to \R{confint()}
<<>>=
confint(mirex.lm1)
@

As with SLR, it is useful to visualize the results of the model with a fitted line plot.  In IVR, though, the fitted line plot will consist of lines for each group in the analysis.  The fitted line plot for an IVR is obtained by submitting the \R{lm} object to \R{fitPlot()}.  Several aspects of the fitted line plot can be changed with the arguments to this function\footnote{See \R{?fitPlot} for details.}.  A legend can be added to the plot by setting the \R{legend=} argument to a string stating where the legend should be placed(see \R{?legend} for help with this aspect).  For example, the fitted line plot with a legend in the top-left corner would be created with

<<eval=FALSE>>=
fitPlot(mirex.lm1,legend="topleft",ylab="Mirex Concentration",xlab="Weight")
@

Predicted values for IVR models are constructed by submitting the \R{lm} object and a data frame of observed values of the explanatory variables to \R{predict()} or \R{predictionPlot()} as was described for SLR models.  One must make sure, though, that the data frame contains an observed value for each explanatory variable found in the \R{lm} object.  For example, the predicted value, with a 95\% prediction interval, for a 3 kg coho salmon is found with

<<>>=
predict(mirex.lm1,data.frame(weight=3,species="coho"),interval="prediction")
@

Similarly, the predicted values, with 95\% prediction intervals, for 5 kg chinook AND a 3 kg coho salmon are found with

<<>>=
predict(mirex.lm1,data.frame(weight=c(5,3),species=c("chinook","coho")),interval="prediction")
@


\section{Hypothesis Testing}
\subsection{Overall F-Test}
As with the other linear models considered thus far, IVR models can be cast as a comparison of two models.  The \emph{ultimate full model} is the model that contains all of the explanatory variables of interest (i.e., the covariate plus all indicator and interaction variables).  The \emph{ultimate simple model} is the same as it was with ANOVA and SLR; i.e., the model with no explanatory variables and just a constant at $\alpha$ or, equivalently, $\overline{Y}$.  The so-called overall F-test is a comparison of the ultimate full to the ultimate simple model.

\defn{Ultimate Full Model}{The regression model that contains ALL of the explanatory variables of interest.}

\vspace{-12pt}
\defn{Ultimate Simple Model}{The regression model that contains NONE of the explanatory variables of interest.}

The overall ANOVA table for IVRs considers three sources of variability -- regression, residual, and total.  The $SS_{total}$ is, as always, found by summing the squared residuals from the ultimate simple model\footnote{With corresponding $df_{total}$ of $n-1$, consistent with all other linear models considered thus far.}.  The $SS_{residual}$ is the sum of squared residuals from the full model, with $df_{residual}$ equal to the number of individuals minus the number of parameters in the model.  As with the SLR, $SS_{regression}$ is the difference between $SS_{total}$ and $SS_{residual}$ and represenents the improvement in fit from using the ultimate full versus the ultimate simple model.  Correspondingly, the $df_{regression}$ are the difference between the number of parameters in the ultimate simple (always 1) and full models.

The MS and F test are computed as before.  The $MS_{regression}$ and $MS_{residual}$ have the same interpretation as before -- $MS_{regression}$ is the amount of variability that the full model explained and $MS_{residual}$ is the amount of variability that the full model could not explain.  However, in IVR, the $MS_{regression}$ is how much variability \textbf{ALL} of the explanatory variables combined could explain.  In other words, the \emph{COMBINED} effect of all explanatory variables is considered in the overall ANOVA F-test for IVR.

\warn{The $MS_{Regression}$ in IVR is a measure of the combined variability in the response variable that IS explained by \emph{ALL} of the explanatory variables being considered.}

Thus, a significant F test indicates that a substantial portion of the variability in the response variable is explained by all of the explanatory variables combined.  Importantly, a significant F test indicates that there is a significant relationship between the response and some combination, including potentially only one, of the explanatory variables.  The F test does not immediately indicate which, or which combination, of the explanatory variables forms the significant relationship.  Methods for determining which variables are significant are considered in the next two sections.  As a general rule, the overall F test should be consulted first to determine if some significant relationship exists.  If no significant relationship exists, then one need not examine specific relationships further.  If a significant relationship is found, then the methods of the next sections can be considered.

\warn{A significant F test in an IVR indicates that at least one of the explanatory variables is important for explaining the variability of the response variable.}

\vspace{-12pt}
\warn{An overall F-test (and corresponding p-value) indicates whether or not there is a significant relationship between the response variable and all of the explanatory variables taken as a group.  Post-hoc methods are required to further dissect the specific nature of the relationship.}


\subsection{Partial F-Tests} \label{sect:IVRPartialFTests}
Not every model comparison related to IVR needs to involve the ultimate simple and full models.  For example, it may be of interest to determine if the two lines in the mirex concentration data corresponding to the coho and chinook salmon are \emph{coincident}.  In other words, do the lines relating mirex concentration to weight have the same intercept and slope for coho and chinook salmon.

The hypotheses related to these questions can be written in a variety of equivalent ways.  First, the hypotheses can be written in words as

\[ \begin{split}
  H_{O}&: \text{``The coho and chinook lines are coincident''} \\
  H_{A}&: \text{``The coho and chinook lines are NOT coincident''} \\
\end{split} \]

Second, the hypotheses can be written in terms of coefficients as (referring back to \eqref{eqn:IVRSalmonModel1})

\[ \begin{split}
  H_{O}&: \delta_{1} = \gamma_{1} = 0 \\
  H_{A}&: \text{at least one of $\delta_{1}$ or $\gamma_{1}$ is different than 0} \\
\end{split} \]

Finally, and most usefully, the hypotheses can be written in terms of models as

\[ \begin{split}
  H_{O}&: \mu_{mirex|\cdots} = \alpha+\beta_{1}weight \\
  H_{A}&: \mu_{mirex|\cdots} = \alpha+\beta_{1}weight+\delta_{1}speciescoho+\gamma_{1}speciescoho*weight \\
\end{split} \]

Thus, if, for example, significant evidence is found to support the alternative model then there is also significant evidence for ``at least one of $\delta_{1}$ or $\gamma_{1}$ is different than 0'' and ``the coho and chinook lines are NOT coincident.''

Models, and their corresponding hypotheses, can be compared by plugging the $SS_{residuals}$ from the competing models into \eqref{eqn:FModelGeneral} to generate an F test statistic.  This F test statistic, similar to all other F test statistics based on model comparisons, is a ratio of the variability that WAS explained by the full model to the variability that was not explained by the full model.  A ``large'' F indicates that the full model explains significantly more of the variability in the response variable then the simple model does.

This process can be illustrated with the mirex concentration data.  First, the ultimate full model (which corresponds to $H_{A}$ in this example) is fit and the SS are extracted with \R{anova()} as follows

<<echo=-1>>=
a1 <- anova(mirex.lm1)
anova(mirex.lm1)
@

The $df$ and $SS$ of interest, at this point, are in the ``Residuals'' row of the output.  Thus, $RSS_{Full}$=\Sexpr{formatC(a1["Residuals","Sum Sq"],format="f",digits=3)}, $df_{Full}$=\Sexpr{a1["Residuals","Df"]}, and $RMS_{Ultimate Full}$=\Sexpr{formatC(a1["Residuals","Mean Sq"],format="f",digits=4)}.  Second, the simple model (which corresponds to $H_{O}$) is fit and the SS are extracted with

<<echo=-c(3,4)>>=
mirex.lm2 <- lm(mirex~weight,data=Mirex)
anova(mirex.lm2)
a2 <- anova(mirex.lm2)
F1 <- (a2["Residuals","Sum Sq"]-a1["Residuals","Sum Sq"])/(a2["Residuals","Df"]-a1["Residuals","Df"])/a1["Residuals","Mean Sq"]
@

Thus, $RSS_{Simple}$=\Sexpr{formatC(a2["Residuals","Sum Sq"],format="f",digits=3)} and $df_{Simple}$=\Sexpr{a2["Residuals","Df"]}.  Third, these values are put into \eqref{eqn:FModelGeneral} as such

\[ F = \frac{\frac{\Sexpr{formatC(a2["Residuals","Sum Sq"],format="f",digits=3)}-\Sexpr{formatC(a1["Residuals","Sum Sq"],format="f",digits=3)}}{\Sexpr{a2["Residuals","Df"]}-\Sexpr{a1["Residuals","Df"]}}}{\Sexpr{formatC(a1["Residuals","Mean Sq"],format="f",digits=4)}} = \Sexpr{formatC(F1,format="f",digits=2)} \]

With \Sexpr{a2["Residuals","Df"]-a1["Residuals","Df"]} and \Sexpr{a1["Residuals","Df"]} df, the p-value is \Sexpr{kPvalue(distrib(F1,type="f",df1=a2["Residuals","Df"]-a1["Residuals","Df"],df2=a1["Residuals","Df"],distrib="f",lower.tail=FALSE,plot=FALSE),include.p=FALSE)}\footnote{The p-value can be computed with \R{distrib()}.  In this case, \R{distrib(\Sexpr{formatC(F1,format="f",digits=4)},distrib="f",df1=\Sexpr{a2["Residuals","Df"]-a1["Residuals","Df"]},df2=\Sexpr{a1["Residuals","Df"]},lower.tail=FALSE)}.}.  This p-value indicates that the full model does NOT fit the data ``better'' than the simple model and, thus, the lines for the chinook and coho salmon are likely coincident.  These results suggest that the chinook and coho salmon have statistically similar slopes and intercepts.

\defn{Coincident Lines Test}{An F test to determine if all groups in an IVR analysis can be described by the exact same regression line.}

\vspace{-12pt}
\warn{A coincident lines test is conducted by comparing the ultimate full model to the model that contains only the covariate (i.e., no indicator or interaction variables are included).}

If a group of lines are found to be NOT coincident then the analyst should attempt to determine if the lines have equal slopes or not.  A parallel lines test is conducted by comparing the ultimate full model to the model that contains no interactions between the covariate and the indicator variables.  A parallel lines test is not needed in this example because the lines were found to be coincident.  However, the analysis below will be used with these data to illustrate a parallel lines test.  In this example, the model hypotheses for the equal slopes test are

\[ \begin{split}
  H_{O}&: \mu_{mirex|\cdots} = \alpha+\beta_{1}weight+\delta_{1}speciescoho \\
  H_{A}&: \mu_{mirex|\cdots} = \alpha+\beta_{1}weight+\delta_{1}speciescoho+\gamma_{1}speciescoho*weight \\
\end{split} \]

The F test for comparing these models still uses the results from the fit of the ultimate full model -- i.e., $RSS_{Full}$=\Sexpr{formatC(a1["Residuals","Sum Sq"],format="f",digits=3)}, $df_{Full}$=\Sexpr{a1["Residuals","Df"]}, and $RMS_{Ultimate Full}$=\Sexpr{formatC(a1["Residuals","Mean Sq"],format="f",digits=4)}.  However, the simple model is different, and its results are found with

<<echo=-c(3,4)>>=
mirex.lm3 <- lm(mirex~weight+species,data=Mirex)
anova(mirex.lm3)
a3 <- anova(mirex.lm3)
F2 <- (a3["Residuals","Sum Sq"]-a1["Residuals","Sum Sq"])/(a3["Residuals","Df"]-a1["Residuals","Df"])/a1["Residuals","Mean Sq"]
@

Thus, $RSS_{Simple}$=\Sexpr{formatC(a3["Residuals","Sum Sq"],format="f",digits=3)} and $df_{Simple}$=\Sexpr{a3["Residuals","Df"]} and the resultant F test statistic is

\[ F = \frac{\frac{\Sexpr{formatC(a3["Residuals","Sum Sq"],format="f",digits=3)}-\Sexpr{formatC(a1["Residuals","Sum Sq"],format="f",digits=3)}}{\Sexpr{a3["Residuals","Df"]}-\Sexpr{a1["Residuals","Df"]}}}{\Sexpr{formatC(a1["Residuals","Mean Sq"],format="f",digits=4)}} = \Sexpr{formatC(F2,format="f",digits=2)} \]

With \Sexpr{a3["Residuals","Df"]-a1["Residuals","Df"]} and \Sexpr{a1["Residuals","Df"]} df the p-value is \Sexpr{kPvalue(distrib(F2,type="f",df1=a3["Residuals","Df"]-a1["Residuals","Df"],df2=a1["Residuals","Df"],distrib="f",lower.tail=FALSE,plot=FALSE))}, which provides weak evidence that the full model is ``better'' than the simple model.  If an $\alpha$ of 0.05 is being used then one would conclude that the coho and chinook salmon have equal slopes.

\defn{Parallel Lines Test}{An F test to determine if all groups in an IVR analysis can be described by regression lines with the exact same slope.}

\vspace{-12pt}
\warn{A parallel lines test is conducted by comparing the ultimate full model to the model that contains the covariate and indicator variables (i.e., no interaction variables with the quantitative variable are included).}

If the groups in an IVR are found to be parallel (i.e., have equal slopes) then the analyst should proceed to determine if the groups also have statistically equal-intercepts.  \emph{It must be noted here, though, that a test of equal-intercepts is relevant only if the lines are known or have been found to be parallel.}  The reasoning for this warning can be illustrated in at least two ways.  First, recall that the difference in intercepts come from examination of the coefficients on the indicator variables.  The indicator variables stem from one original factor variable and, thus, can be thought of as ``main effects.''  If a parallel lines test indicates non-parallel lines then it has been found that the interaction term is an important contributor to the model.  As was learned with a two-way ANOVA, if an interaction exists in the model then the main effect terms should not be interpreted.

\defn{equal-intercepts Test}{An F test to determine if all groups in an IVR analysis can be described by regression lines with the exact same intercept \emph{given that the lines have equal slopes}.}

\vspace{-12pt}\warn{An equal-intercepts test is only appropriate if the lines are known or have been found to be parallel.}

Second, if the lines are not parallel then the significance of the intercept term depends on the relative magnitude of the slopes and ``how far'' the observed data is from $X=0$.  For example, the intercepts in \figref{fig:IVRInterceptIssue}-Left are statistically different.  However, if the center of the observed data was on $X=0$ as in \figref{fig:IVRInterceptIssue}-Right then the intercepts would not be statistically different.  In summary, the test of equal-intercepts is not a useful test if the slopes differ between the groups.

<<IVRInterceptIssue, echo=FALSE, fig.cap="Representation of two sub-model fits with non-parallel lines.  The left plot illustrates a situation where $X=0$ is at the left margin of the observed data.  The right plot illustrates a situation where $X=0$ is at the center of the observed data.  Overall, the two plots illustrate how the difference in intercepts depends on where $X=0$ is relative to the two fitted lines.", fig.pos="h">>=
 y1 <- c(0,0.2); y2 <- c(0.8,1)
 x <- c(0,1)
 plot(x,c(y1[2],y2[1]),xlab="X",ylab="Y",type="l",lwd=2,xlim=c(0,1),ylim=c(0,1),cex.lab=1.25,xaxt="n",yaxt="n")
 lines(x,c(y1[1],y2[2]),lwd=2,col="red")
 axis(1,0,0); abline(v=0,lty=3)
 x <- c(-0.5,0.5)
 plot(x,c(y1[2],y2[1]),xlab="X",ylab="Y",type="l",lwd=2,xlim=c(-0.5,0.5),ylim=c(0,1),cex.lab=1.25,xaxt="n",yaxt="n")
 lines(x,c(y1[1],y2[2]),lwd=2,col="red")
 axis(1,0,0); abline(v=0,lty=3)
@

On the other hand, if the lines are parallel then the equal-intercepts test is a very important test.  With parallel lines the difference in intercepts is a measure of the vertical difference between the lines for every value of the covariate.  Thus, the difference in intercepts is a measure of the difference in the response variable between the groups when the covariate is held constant.  In these situations, the equal-intercepts test tests whether this constant difference is significantly different than 0 or not.

\warn{With parallel lines the equal-intercepts test tests whether the the mean of the response variable when the quantitative variable is held constant differs between the groups.}

The model hypotheses for the equal-intercepts test, assuming parallel lines, are,

\[ \begin{split}
  H_{O}&: \mu_{mirex|\cdots} = \alpha+\beta_{1}weight \\
  H_{A}&: \mu_{mirex|\cdots} = \alpha+\beta_{1}weight+\delta_{1}speciescoho \\
\end{split} \]

As the full model does not have an interaction term then it clearly states that this test of equal-intercepts follows knowing or testing that the slopes are parallel.

<<echo=FALSE>>=
F3 <- (a2["Residuals","Sum Sq"]-a3["Residuals","Sum Sq"])/(a2["Residuals","Df"]-a3["Residuals","Df"])/a1["Residuals","Mean Sq"]
@

The full model in these hypotheses is not the ultimate full model and is actually the simple model from the parallel lines test.  Thus, $RSS_{Full}$=\Sexpr{formatC(a2["Residuals","Sum Sq"],format="f",digits=3)} and $df_{Full}$=\Sexpr{a2["Residuals","Df"]}.  However, $RMS_{Ultimate Full}$=\Sexpr{formatC(a1["Residuals","Mean Sq"],format="f",digits=4)} still holds.  The simple model in this case was also the simple model in the coincident lines test.  Thus, $RSS_{Simple}$=\Sexpr{formatC(a3["Residuals","Sum Sq"],format="f",digits=3)} and $df_{Simple}$=\Sexpr{a3["Residuals","Df"]}.  The F test statistic,

\[ F = \frac{\frac{\Sexpr{formatC(a2["Residuals","Sum Sq"],format="f",digits=3)}-\Sexpr{formatC(a3["Residuals","Sum Sq"],format="f",digits=3)}}{\Sexpr{a2["Residuals","Df"]}-\Sexpr{a3["Residuals","Df"]}}}{\Sexpr{formatC(a1["Residuals","Mean Sq"],format="f",digits=4)}} = \Sexpr{formatC(F3,format="f",digits=2)} \]

With \Sexpr{a2["Residuals","Df"]-a3["Residuals","Df"]} and \Sexpr{a1["Residuals","Df"]} df the p-value is \Sexpr{kPvalue(distrib(F3,df1=a2["Residuals","Df"]-a3["Residuals","Df"],df2=a1["Residuals","Df"],distrib="f",lower.tail=FALSE,plot=FALSE))}.  Thus, if the lines are considered to be parallel then there does not appear to be a significant difference between the intercepts for the chinook and coho salmon lines.

The default hypothesis test comparing the ultimate full and ultimate simple models is called the overall F-test.  The hypothesis tests that compare a full and a simple model of which at least one is not an ultimate model is called a \emph{partial F-test}.  Partial F-tests are very useful for examining hypothesis tests that require combining groups of coefficients (e.g., ``coincident lines'' and ``parallel lines'' tests).

\defn{Overall F Test}{The F-test that compares the ultimate full to the ultimate simple model.}

\vspace{-12pt}
\defn{Partial F Test}{An F-test where at lease one of the full and simple models is not the ultimate full or simple model.}

\subsubsection*{Partial F Tests in R}
Most of the F tests just described can all be computed efficiently by submitting the saved \R{lm} object from the fit of the \emph{ultimate full model} to \R{anova()}.  For example, a close examination of

<<>>=
anova(mirex.lm1)
@

shows that the p-values for the parallel lines and equal-intercepts tests (assuming parallel lines) are found in the lines corresponding to the interaction and factor variables, respectively.  This a general result; i.e., these two p-values will always be found in these two lines.  As the equal interecepts test assumes parallel lines the equal-intercepts test p-value should never be used if the parallel lines p-value is less than $\alpha$ (i.e., indicating non-parallel lines).

\warn{The p-value for the parallel lines test is found in the row from \R{anova()} labelled with the interaction variable.}

\vspace{-12pt}
\warn{The p-value for the equal-intercepts test (assuming parallel lines) is found in the row from \R{anova()} labelled with the factor variable.}

\vspace{-12pt}
\warn{The equal-intercepts test p-value should NOT be interpreted if the parallel lines test p-value is less than $\alpha$.}

The coincident lines test p-value is not found in the output from \R{anova()} as described above.  However, when \R{anova()} receives multiple \R{lm} objects it performs the general F-test described above.  Thus, the partial F test can be constructed by including the \R{lm} objects corresponding to the simple and full models (in that order) to \R{anova()}.  For example, the partial F ``coincident lines'' test for the mirex concentration data is computed with

<<>>=
anova(mirex.lm2,mirex.lm1)
@


\subsection{Coefficients t-Tests}
Most computer programs, including R, construct default hypothesis tests for the coefficients from a model fit.  The hypothesis tests about individual model coefficients are constructed in the same manner as described for SLR \chaprefp{chap:LMRegression1}.  Specifically, these default tests test that a coefficient is not equal to zero with the t test statistic found in \eqref{eqn:tTestStat}.  The test statistics and p-values for the coefficients from fitting \eqref{eqn:IVRSalmonModel1} are found with \R{summary()}.

\warn{The default t-tests for the model coefficients are used to test $H_{A}:\beta_{i}\neq0$.}

Despite the familiarity and simplicity of these calculations, one must be very careful when interpreting the results.  These tests test the significance of a variable in describing the variability of the response variable \emph{after all other explanatory variables have been included in the model.}  In other words, these tests test the importance of a particular explanatory variable adjusted for every other variable in the model.  For example, the test of the \var{speciescoho} coefficient in \eqref{eqn:IVRSalmonModel1} tests whether the intercepts differ between chinook and coho salmon assuming that \var{weight:speciescoho} is in the model.  In other words, this test tests whether the intercepts differ assuming that the slopes are unequal.  This is problematic because it is testing a ``main effect'' after an interaction effect is in the model.  Thus, in general, the test statistics and p-values corresponding to the model coefficients are not useful.

\warn{The default t-tests for the model coefficients test the importance of an explanatory variable after all other explanatory variables are included in the model.}

\vspace{-12pt}
\warn{The default t-tests for the model coefficients are generally inappropriate.}

The actual coefficient values are useful, however, because they are estimates of the intercept and slope for the reference group or for the difference in intercepts and slopes as described in previous sections.  Thus, model coefficient values will be interpreted but their significance will not be determined from the default t-tests.  Methods for testing which slopes are different or which intercepts are different will be described in the next section.



\section{Multiple Comparisons} \label{sect:IVRMultComparisons}
\subsubsection*{Comparing Slopes}
To motivate the topics of this section, consider a change in the intent for the mirex concentration in salmon data.  Suppose that interest is in determining if the relationship between mirex concentration in the tissue and the weight of the salmon differs among the six collection years, rather than between the two species.  The following initial commands were used to partially address this interest by performing the parallel lines test and, if appropriate, the equal-intercepts test\footnote{The second line is used to convert \var{year} from a numeric to a factor variable.}.

<<>>=
data(Mirex)
Mirex$year <- factor(Mirex$year)
mirex.lma <- lm(mirex~weight*year,data=Mirex)
anova(mirex.lma)
@

The interaction term p-value (\Sexpr{kPvalue(anova(mirex.lma)["weight:year","Pr(>F)"])}) provides very strong evidence that the six lines are not parallel.  Because the lines are not parallel, the equal-intercepts test (second line of output) was not interpreted.  The next question after a result such as this is to ask which lines have different slopes -- does just one line have a different slope, do all lines have different slopes, etc.  An examination of the fitted-line plot for this model \figrefp{fig:IVRSalmonYearFittedLinePlot} suggests that the first four years have similar slopes, 1996 has a shallower slope, and 1999 has an even shallower slope.  However, it is impossible to determine which slopes are significantly different from this graph.

<<IVRSalmonYearFittedLinePlot, echo=FALSE, fig.cap="Fitted line plot for the salmon mirex concentration by year linear model results.", fig.pos="h">>=
 fitPlot(mirex.lma,legend="topleft",main="")
@

One method for identifying \textbf{\emph{some}} differences is to examine the t-tests for the interaction term coefficients.  It should be noted that examination of these tests is appropriate in this case because the interaction terms are added at the ``end of the model'' and their interpretation ``after all other variables are in the model'' is exactly as was intended.  Thus, for example, the coefficient test results for the ultimate model fit to these data are

<<>>=
summary(mirex.lma)
@

The last two p-values in this list suggest that the slope for 1996 and the slope for 1999 are significantly different from the slope for 1977, the reference year.  The three p-values prior to the last two indicate that the slopes for 1982, 1986, and 1992 are not significantly different from the slope for 1977.

While these results are useful for comparing slopes to 1977, they do not allow comparison of slopes between any pair of years that does not include 1977.  For example, it is impossible from these results to determine if the slopes for 1982 and 1986 differ.  Slope comparisons to other reference years can be obtained by changing the reference level for \var{year} with the \R{levels=} argument in \R{factor()}.  However, this is a tedious process because the reference level would have to be changed multiple times to obtain tests for all possible pairs of years\footnote{The number of models that would need to be fit (or the number of ``re-levelings'') that are required is one less than the number of groups in the analysis.}.

In addition to the tedium of ``re-leveling'' to see all possible comparison pairs, this process is also subject to inflation of the experimentwise error rate due to the ``multiple comparisons problem'' (see \sectref{sect:MultComp}).  Unfortunately, a method such as Tukey's HSD method does not exist for comparing multiple slopes.  Thus, a generic multiple comparison method must be used.

One such method is the \emph{Bonferroni method}.  The Bonferroni method protects against experimentwise error rate inflation by multiplying each p-value by the total number of comparisons being made.  The larger p-value makes it more difficult to reject an individual $H_{0}$ and, thus, the experiment-wise error rate does not increase dramatically with multiple tests.  However, the Bonferroni method is too conservative in its control of the experiment-wise error rate causing it to be much below a desired value when large numbers of tests are analyzed.  This conservatism results in a lack of statistical power and, thus, a difficulty in finding real differences in treatment means when they exist.

\warn{The Bonferroni method of correcting for multiple comparisons is very conservative resulting in a dramatic loss of power.}

Several modifications of the Bonferroni method have been suggested in an attempt to maintain power while still controlling the experimentwise error rate.  Some less conservative and more powerful methods are those described by \cite{Holm1979}, \cite{Hochberg1988}, \cite{Hommel1988}, and \cite{Rice1989}.  These methods fall under the heading of \emph{sequential Bonferroni methods}\footnote{The \cite{Rice1989} paper introduced the concept of sequential Bonferroni techniques developed in the earlier papers to the ecological and biological literature.}.  The details of these methods will not be discussed here.  However, it should be noted that sequential Bonferroni methods have fallen out of favor with ecologists in the last decade\footnote{See \cite{Perneger1998}, \cite{Moran2003}, \cite{Garcia2004}, and \cite{RobackAskins2005}.}.

\warn{Sequential Bonferroni methods of correcting for multiple comparisons are less conservative than the Bonferroni method.  These methods still result, however, in a substantial loss of power.}

In recent years, the concept of controlling the ``false discovery rate'' (FDR) rather than the experimentwise error rate has become popular, especially in the ecological literature (see \cite{Verhoevenetal2005}).  The FDR is the expected proportion of false discoveries (i.e., rejecting $H_{0}$ when it is in fact true) among the rejected null hypotheses.  Controlling the FDR is a less stringent condition than controlling the experimentwise error rate.  Therefore, methods that use the FDR are more powerful than the Bonferroni-related methods that use the experimentwise error rate.  Methods based on the FDR have been described by \cite{BenjaminiHochberg1995}, \cite{BenjaminiYekutieli2001}, and \cite{Storey2002}.  The details of these methods will not be discussed here either but the interested reader is directed to the very readable summary by \cite{Verhoevenetal2005}.  Because of the increased power related to FDR-based methods, it is suggested that you use these methods when comparing slopes among a large number of groups.

\defn{False Discovery Rate}{the expected proportion of false discoveries among the rejected null hypotheses.}

\vspace{-12pt}
\warn{Methods that control the ``false discover rate'' rather than the ``experimentwise error rate'' are more powerful than Bonferroni or sequential Bonferroni methods of multiple comparisons.}

The Bonferroni, sequential Bonferroni, or FDR methods work by multiplying the individual comparison p-values by a factor greater than 1.  This ``adjusted p-value'' is then compared to the desired $\alpha$ for the desired experimentwise error rate or false discovery rate.  For example, the following results summarize the comparisons of all slopes among all pairs of groups in the mirex concentration by year analysis using the FDR control method

<<echo=FALSE, background='white'>>=
print(data.frame(compSlopes(mirex.lma)$comparisons),row.names=FALSE)
@

These results (in the \var{adj.p} column) indicate that the slopes for 1977, 1982, 1986, and 1992 are all equal, the slopes for 1996 and 1999 are significantly lower than the slopes for the other four years, and the slopes from 1996 and 1999 are equal.  Thus, it appears that the slope between mirex concentration in the tissue and fish weight remained constant from 1977 to 1992, declined significantly in 1996, and remained lower in 1999.

\subsubsection*{Comparing Intercepts}
If the lines are all parallel among the groups but the intercepts are different, then the same multiple comparisons problem is encountered.  However, in this instance the differences among all pairs of intercepts can be analyzed with a slight modification of Tukey's HSD method.  The modification is that Tukey's procedure must be constructed with the so-called \emph{adjusted values}.  The adjusted values in an IVR with parallel lines are the original observed values of the response variable adjusted as if all individuals had the exact same value of the covariate.  Typical common values to be used for the covariate are 0 and the mean of the covariate.

A conceptual visualization of this ``adjustment'' process is to fit the parallel lines model to the observed data (\figref{fig:IVRAdjustedValues}-Top Left), imagine tilting the fitted line plot from this model fit such that the fitted lines have a slope of zero (\figref{fig:IVRAdjustedValues}-Top Right), and then ``compressing'' all of the points left-to-right so that they are all centered on $X=0$ or $X=\overline{X}$ (\figref{fig:IVRAdjustedValues}-Bottom Left).  A one-way ANOVA model is then used to determine if the means of the adjusted values are different (\figref{fig:IVRAdjustedValues}-Bottom Right)\footnote{The results of this one-way ANOVA model should indicate a significant difference among mean adjusted values if the equal-intercepts hypothesis was rejected.}.  More importantly, Tukey's HSD method can then be applied to the adjusted data to determine which adjusted means and, hence which intercepts, are significantly different.

<<IVRAdjustedValues, echo=FALSE, fig.width=7, fig.height=7, out.width='.8\\linewidth', fig.cap="Plots illustrating the process of adjusting observed values to a common scale to allow the use of Tukey's HSD method for detecting differences in intercepts for parallel regression lines.  The process is detailed in the text but proceeds left-to-right and then top-to-bottom on this graph.", fig.pos="h">>=
 par(mar=c(3.5,3.5,1,1), mgp=c(2.5,0.75,0),mfrow=c(2,2),las=1,tcl=-0.2)
 Mirex1 <- Subset(Mirex, year!="1996" & year!="1999")
 mirex1.lmb <- lm(mirex~weight+year,data=Mirex1)
 fitPlot(mirex1.lmb,main="",legend="topleft",ylab="Mirex Concentration",xlab="Weight",pch=1)
 Mirex1$adjmirex <- predict(mirex1.lmb,data.frame(weight=rep(0,length(Mirex1$weight)),year=Mirex1$year))+mirex1.lmb$residuals
 fitPlot(lm(adjmirex~weight+year,data=Mirex1),main="",legend="bottomleft",ylab="ADJUSTED Mirex Concentration",xlab="Weight",pch=1)
 mns <- tapply(Mirex1$adjmirex,Mirex1$year,mean)
 clrs <- chooseColors("rich",4)
 plot(Mirex1$adjmirex~rep(0,length(Mirex1$adjmirex)),col=clrs[as.numeric(Mirex1$year)],xaxt="n",ylab="ADJUSTED Mirex Concentration",xlab="Index")
 axis(1,0,0); legend("topleft",legend=levels(Mirex1$year),col=clrs,pch=1)
 points(rep(0,length(mns)),mns,pch=95,col=clrs,cex=3)
 plot(Mirex1$adjmirex~as.numeric(Mirex1$year),col=clrs[as.numeric(Mirex1$year)],xaxt="n",ylab="ADJUSTED Mirex Concentration",xlab="Year",xlim=c(0.5,4.5))
 axis(1,1:4,levels(Mirex1$year))
 points(1:4,mns,pch=95,col=clrs,cex=3)
@
\vspace{9pt}
As an example analysis of the intercepts, consider the mirex concentration data JUST for the four years from 1977 to 1992 (the years for which the lines were parallel).  This reduced data set is obtained from the larger data set with,

<<>>=
Mirex1 <- Subset(Mirex, year!="1996" & year!="1999")
@

The parallel line test and equal-intercepts tests for this reduced data set are obtained with,

<<>>=
mirex1.lma <- lm(mirex~weight*year,data=Mirex1)
anova(mirex1.lma)
@

The interaction term p-value (\Sexpr{kPvalue(anova(mirex1.lma)["weight:year","Pr(>F)"],digits=3)}) confirms the conclusion that the slopes for these four years are all equal.  The indicator variable p-value (\Sexpr{kPvalue(anova(mirex1.lma)["year","Pr(>F)"],digits=3)}) indicates that at least one pair of the intercepts for these four years are different.  The results from the Tukey HSD method applied to the adjusted data are

<<echo=FALSE, background='white', warning=FALSE>>=
print(data.frame(compIntercepts(mirex1.lma)$comparisons),row.names=FALSE)
@

These results suggest that the intercept for 1977 is significantly higher than the intercepts for the other three years. The intercepts for the other three years are all statistically equal.  These results, combined with the parallel line test from above, suggest that the regression of mirex concentration on fish weight is coincident for 1982, 1986, and 1992.  In addition, the adjusted mean mirex concentration in 1977 is higher than the adjusted mean mirex concentration in 1982, 1986, and 1992 for all salmon weight values.  Thus, the \emph{relationship} between mirex concentration and salmon weight did not change from 1977 to the following three years but the mirex concentrations declined significantly no matter the weight of the fish.

\subsubsection*{Multiple Comparisons in R}
The table of results for comparing all possible pairs of slopes can be obtained by submitting the saved \R{lm} object containing the ultimate full model to \R{compSlopes()}.  This function defaults to using the preferred \R{"fdr"} method\footnote{Oher methods of control can be used by changing the \R{control=} argument to one of \R{"bonferroni"}, \R{"holm"}, \R{"hochberg"}, or \R{"hommel"}.}.  The desired level at which to control the experimentwise error or the false discovery rate can be changed by including a decimal value in the \R{alpha=} argument.  The FDR control results for the mirex concentration by year example shown above were obtained with

<<eval=FALSE>>=
compSlopes(mirex.lma)
@

If the lines are found to be parallel and there is some difference in the intercepts then the specific differences in intercepts can be found by submitting the saved \R{lm} object to \R{compIntercepts()}\footnote{The \R{lm} object should not contain an interaction term as testing the intercepts is only appropriate on parallel lines.  However, \R{compIntercepts()} will adjust, though a warning will be issued, to using a model without an interaction term if a model with an interaction term is supplied.}.  This function converts the observed data to adjusted values, fits the required one-way ANOVA model, and then uses Tukey's procedure to identify any differences in adjusted values.  The Tukey HSD results shown above for the mirex concentration example for the four years between 1977 and 1992 were obtained with

<<eval=FALSE>>=
mirex1.lmb <- lm(mirex~weight+year,data=Mirex1)
compIntercepts(mirex1.lmb)
@


\section{Assumptions \& Transformations}
The assumptions for an IVR are the same as those for an SLR -- independence, linearity, homoscedasticity, normality, and no outliers \sectrefp{sect:SLRAssumptions}.  Methods for assessing the validity of these assumptions in an IVR are also essentially the same as those used for an SLR.  In other words, fitted-line and residual plots will be used to examine linearity and assess homoscedasticity, an Anderson-Darling test and a histogram of the residuals will be used to assess the normality of residuals, and a residual plot and an outlier test will be used to diagnose outliers and influential points.

\warn{The assumptions of an IVR are the same as the assumptions for an SLR.}

\vspace{-12pt}
\warn{Methods for assessing the assumptions for an IVR are the same as the methods use for an SLR.}

The only point of note for these analyses is that the assumptions should be assessed on the ultimate full model.  If the assumptions are not met on the ultimate full model then transformations for the response and the covariate\footnote{If the covariate is transformed then any interaction terms with the covariate should be recreated using the transformed covariate.} should be considered such that a transformed version of the ultimate full model meets the assumptions\footnote{The methods and rules for transforming these variables in an IVR are the same as those used for an SLR (see \sectref{sect:SLRTransformations}).}.  Once an ultimate full model is found where all of the assumptions are met, then it is assumed that all of the assumptions will be met for any model that is a nested subset of the ultimate full model.  In other words, the assumptions do not need to be re-checked if non-significant explanatory variables are removed from the ultimate model.

\warn{The assumptions are assessed for the ultimate full model.}

\vspace{-12pt}
\warn{Assumptions do not need to be assessed for simpler models that are nested within the ultimate full model if the assumptions were met for the ultimate full model.}


\section{Example Analyses}
\subsection{Crab Claws}
\subsubsection*{Introduction}
As part of a study of the effects of predatory intertidal crab species on snail populations, \cite{YamadaBoulding1998} measured the mean closing forces and the propodus heights of the claws on several individuals of three species of crabs -- \emph{Hemigrapsus nudus}, \emph{Lophopanopeus bellus}, and \emph{Cancer productus}.  In one part of their research they hypothesized that the closing force of claws of species that were mollusc specialists (\emph{Lophopanopeus bellus} and \emph{Cancer productus}) would be greater than the closing force of claws of species that were mollusc generalists (\emph{Hemigrapsus nudus}).  The closing force would be adjusted for the size of the claw because of a suspected positive relationship between the size of the claw and the closing force.

\subsubsection*{Data Collection}
Claw closing forces were measured with an \emph{in vivo} ``calibration device'' that consisted of a piece of heavy wire bent into an $\mho$ shape and a strain gauge glued to a piece of 0.76 mm steel shimstock and hot-melt glued to the straight edge of the wire.  A linear relationship was found between the force exerted on the calibrating device by known weights and the output of the Wheatstone bridge containing the device's strain gauge.  Claw closing forces were measured by positioning the mid-dactyl and mid-propodus fingers of crab's claws into the upper and lower wire loops of the strain gauge and waiting for the crab to ``attack'' so that it closed its claw and pulled the loops together.  Crabs were selected from the three species possessing propodus heights between 4.5 and 13 mm.  Both right and left claws of 11-15 individuals per species were measured repeatedly and the highest reading per crab was recorded.

The data file to be analyzed consists of the following variables and their meanings:
\begin{Itemize}
  \item \var{spec}: a factor with levels \var{cp}, \var{hn}, and \var{lb} corresponding to abbreviations for the three crab species.
  \item \var{propodus}: measurement of claw propodus height (mm).
  \item \var{force}: measurement of claw closing force.
\end{Itemize}
In the models, \var{spec} will be converted to indicator variables of the form \var{spechn} and \var{speclb}.  In addition, interaction variables between \var{propodus} and the indicator variables from \var{spec} will be constructed.

\subsubsection*{EDA \& Assumption Checking}
<<echo=FALSE>>=
 CrabClaw <- read.csv("data/CrabClaw.csv")
 crab.lm1 <- lm(force~propodus*spec,data=CrabClaw)
 crab.ad1 <- adTest(crab.lm1$residuals)
 CrabClaw$sqrt.force <- sqrt(CrabClaw$force)
 crab.lm2 <- lm(sqrt.force~propodus*spec,data=CrabClaw)
 crab.ad2 <- adTest(crab.lm2$residuals)
 crab.out2 <- outlierTest(crab.lm2)
@
The initial ultimate full model fit to these data was
\[ \mu_{force|propodus,spec} = \alpha + \beta_{1}propodus  + \delta_{1}spechn + \delta_{2}speclb + \gamma_{1}spechn*propodus + \gamma_{2}speclb*propodus \]

The fit of this model exhibited a small heteroscedasticity \figrefp{fig:IVRCrabResidPlot} but approximately normal residuals (Anderson-Darling \Sexpr{kPvalue(crab.ad1$p)}).  Possible transformations for the response variable were considered through a trial-and-error method.  A square-root transformation of closing force resulted in linearity and homoscedasticity \figrefp{fig:IVRCrabResidPlot}, approximately normal residuals (\Sexpr{kPvalue(crab.ad2$p)}), and no significant outliers (outlier test \Sexpr{kPvalue(crab.out2$bonf.p)}).  With the assumptions largely met, this analysis will proceed by fitting the following ultimate full model to all indvidiuals in the data set

\[ \mu_{sqrt(force)|\cdots} = \alpha + \beta_{1}propodus + \delta_{1}spechn + \delta_{2}speclb + \gamma_{1}spechn*propodus + \gamma_{2}speclb*propodus \]

<<IVRCrabResidPlot, echo=FALSE, fig.cap="Residual plots (Left) and histograms of residuals (Right) from the fit of the ultimate model on the untransformed data (Top) and the transformed data (Bottom).", fig.pos="h",fig.width=7, fig.height=3.5, out.width='.8\\linewidth'>>=
residPlot(crab.lm1)
residPlot(crab.lm2)
@

\subsubsection*{Results}
The parallel lines test indicated that at least two of the three regression lines of the square root of closing force on propodus height by species had different slopes (\Sexpr{kPvalue(anova(crab.lm2)["propodus:spec","Pr(>F)"])}; \tabref{tab:IVRCrabParallelTest}).  Post hoc multiple comparisons of all pairs of slopes, controlling the false discovery rate at 0.05, indicated that the slope for \emph{Hemigrapsus nudus} was significantly lower than the slopes for the other two species, which were not significantly different \tabrefp{tab:IVRCrabSlopeComp}.  Post hoc multiple comparisons for whether each slope is equal to zero or not indicated that the slopes for \emph{Cancer productus} and \emph{Lophopanopeus bellus} were both significantly greater than zero, whereas the slope for \emph{Hemigrapsus nudus} was not significantly different from zero \tabrefp{tab:IVRCrabSlopeComp}.

\begin{table}[h]
  \centering
  \caption{ANOVA table for ultimate full model using the transformed crab claw data.}\label{tab:IVRCrabParallelTest}
<<echo=FALSE, background="white">>=
kANOVA(crab.lm2)
@
\end{table}

\begin{table}[h]
  \centering
  \caption{Post hoc multiple comparisons of all pairs of slopes (top) and for comparison of each slope to zero (bottom) for the transformed crab claw data.}\label{tab:IVRCrabSlopeComp}
<<echo=FALSE, background="white">>=
compSlopes(crab.lm2)
@
\end{table}

<<echo=FALSE>>=
 CrabClaw1 <- Subset(CrabClaw,spec!="hn")
 crab2.lm1 <- lm(sqrt.force~propodus*spec,data=CrabClaw1)
@
The \emph{Hemigrapsus nudus} crabs were removed from the data set to determine if the intercepts of the parallel relationships between the square root of closing force and propodus height differed between the \emph{Cancer productus} and \emph{Lophopanopeus bellus} crabs.  These results confirmed that these two species have equal slopes (\Sexpr{kPvalue(anova(crab2.lm1)["propodus:spec","Pr(>F)"])}; \tabref{tab:IVRCrabTests2}) and that they have equal-intercepts (\Sexpr{kPvalue(anova(crab2.lm1)["spec","Pr(>F)"])}; \tabref{tab:IVRCrabTests2}).  A visual of the model fit is shown in \figref{fig:IVRCrabFittedLinePlot1}.

\begin{table}[h]
  \centering
  \caption{ANOVA results from fitting the ultimate full model on the transformed crab claw data for only the \emph{Cancer productus} and \emph{Lophopanopeus bellus} crabs.}\label{tab:IVRCrabTests2}
<<echo=FALSE, background="white">>=
kANOVA(crab2.lm1)
@
\end{table}

<<IVRCrabFittedLinePlot1, echo=FALSE, fig.cap="Fitted line plot from the fit of the ultimate model on the transformed data.", fig.pos="h">>=
 fitPlot(crab.lm2,main="",legend="topleft",xlab="Propodus Height",ylab="Sqrt(Closing Force)")
@

\subsubsection*{Conclusion}
These results indicate that the relationship between square root closing force and propodus height is the same for \emph{Cancer productus} and \emph{Lophopanopeus bellus} crabs.  In these two species of crabs, the square root closing force increases as the propodus height increases.  In contrast, there is no observable statistical relationship between square root closing force and propodus height for \emph{Hemigrapsus nudus} crabs.

\subsubsection*{Appendix -- R commands}
\begin{Verbatim}[formatcom=\color{red},xleftmargin=5mm,commandchars=\\\{\}]
> CrabClaw <- read.csv("data/CrabClaw.csv")
> crab.lm1 <- lm(force~propodus*spec,data=CrabClaw)
> adTest(crab.lm1$residuals)
> outlierTest(crab.lm1)
> residPlot(crab.lm1)
> transChooser(crab.lm1)
> CrabClaw$sqrt.force <- sqrt(CrabClaw$force)
> crab1.lm1 <- lm(sqrt.force~propodus*spec,data=CrabClaw)
> adTest(crab1.lm1$residuals)
> outlierTest(crab1.lm1)
> residPlot(crab1.lm1)
> fitPlot(crab1.lm1,main="",legend="topleft",xlab="Propodus Height",ylab="Sqrt(Closing Force)")
> anova(crab1.lm1)
> compSlopes(crab1.lm1)
> CrabClaw1 <- Subset(CrabClaw,spec!="hn")
> crab2.lm1 <- lm(sqrt.force~propodus*spec,data=CrabClaw1)
> anova(crab2.lm1)
\end{Verbatim}


\subsection{Possum Morphometrics}
\subsubsection*{Introduction}
Many Australian mammals, from a wide range of taxonomic groups, exhibit variation in external morphology throughout their geographic distributions.  A notable example is the common brushtail possum (\emph{Trichosurus vulpecula}), which is characterized by marked changes in coat color and body size throughout its distribution in Australia.  Variations in the closely related mountain brushtail possum (\emph{Trichosurus caninus}) have been relatively unstudied.  \cite{Lindenmayeretal1995} reported on the results of a study of the external morphology of the mountain brushtail possum from throughout its known geographic range.  One aspect of their study was to determine if the relationship between the total length and foot length of a possum differed between possums captured near Victoria and those captured elsewhere.

\subsubsection*{Data Collection}
\cite{Lindenmayeretal1995} examined morphometric measurements on each of 104 mountain brushtail possums (\emph{Trichosurus caninus gilby}), trapped at seven sites from Southern Victoria to central Queensland in October and November 1993\footnote{These data are from \R{data(possum)} in the \R{DAAG} package.}.  Possums were trapped in large wire cage traps baited with apple.  A total of 50 traps was used at each site.  All animals captured were sedated with ``Zoletil'' to facilitate the collection of a range of morphometric measurements.  Animals were released at the point of capture when recovery was complete.  Several morphometric measurements were made but only the following variables will be used in this analysis:
\begin{Itemize}
  \item \var{Pop}: a factor with levels \var{Vic} for capture near Victoria and \var{other} for capture near New South Wales or Queensland.
  \item \var{totlngth}: measurement of the total length of the animal (cm).
  \item \var{footlgth}: measurement of the length of the food (mm).
\end{Itemize}
The \var{Pop} variable will be ``re-leveled'' so that possums captured near Victoria will serve as the reference group.  In addition, in the models, \var{Pop} will be converted to an indicator variable of the form \var{Popother} and an interaction variable between \var{footlgth} and the indicator variable will be constructed.

The foot length measurement was missing for one individual (original observation 41) in the original data file.  Because of this, this individual was removed from data file for all subsequent analyses.

\subsubsection*{EDA \& Assumption Checking}
<<echo=FALSE>>=
 library(DAAG)
 data(possum)
 possum$Pop <- factor(possum$Pop,levels=c("Vic","other"))
 possum1 <- Subset(possum,!is.na(totlngth) & !is.na(footlgth),select=c("Pop","totlngth","footlgth"))
 pos.lm1 <- lm(totlngth~footlgth*Pop,data=possum1)
 pos.ad1 <- adTest(pos.lm1$residuals)
 pos.out1 <- outlierTest(pos.lm1)
@
The initial ultimate full model fit to these data was
\[ \mu_{totlngth|footlgth,Pop} = \alpha + \beta_{1}footlgth + \delta_{1}Popother + \gamma_{1}Popother*footlgth  \]

The fit of this model is largely linear and homoscedastic \figrefp{fig:IVROpRFLP1} with approximately normal residuals (Anderson-Darling \Sexpr{kPvalue(pos.ad1$p)}) and no detectable outliers (outlier test \Sexpr{kPvalue(pos.out1$bonf.p)}).  However, there are several points that ``stand out'' from the main cluster \figrefp{fig:IVROpRFLP1} -- individual 31 had a total length much longer than would be expected given its foot length, individuals 39 and 43 each had total lengths much shorter than would be expected given their foot lengths, and individual 41 had a foot length and total length shorter than most of the individuals from Victoria.  It should be noted that all of these individuals are from Victoria and that three of the individuals are the shortest three possums recorded (in terms of total length).  It is impossible to tell if any of these points are in error; however, it is possible that they have a heavy influence on the fitted line for Victoria.  The combined influence of these points will be examined in the results.

<<IVROpRFLP1, echo=FALSE, fig.cap="Residual plot (Left) and fitted line plot (Right) from the fit of the ultimate model on the untransformed possum data.  Four individuals discussed during assumption checking are highlighted.", fig.pos="h">>=
 residPlot(pos.lm1,inclHist=FALSE)
 highlight(rstudent(pos.lm1)~pos.lm1$fitted.values,pts=c(31,39,41,43),col="black")
 fitPlot(pos.lm1,main="",xlab="Foot Length",ylab="Total Length",legend="topleft")
 highlight(possum1$totlngth~possum1$footlgth,pts=c(31,39,41,43),col="black")
@

\subsubsection*{Results}
<<echo=FALSE>>=
pos.lm2 <- lm(totlngth~footlgth+Pop,data=possum1)
@
Possums from Victoria and possums from other locations had similar slopes for the relationship between total length and foot length (parallel lines test \Sexpr{kPvalue(anova(pos.lm1)["footlgth:Pop","Pr(>F)"])}; \tabref{tab:IVROpTest1}).  The estimated common slope represents an average \Sexpr{formatC(confint(pos.lm2)["footlgth",1],format="f",digits=2)} to \Sexpr{formatC(confint(pos.lm2)["footlgth",2],format="f",digits=2)} cm increase in total length for a 1 mm increase in foot length.  Assuming parallel lines, the relationship between total length and foot length for possums from Victoria and those from other areas had different intercepts (equal-intercepts test \Sexpr{kPvalue(anova(pos.lm1)["Pop","Pr(>F)"])}; \tabref{tab:IVROpTest1}).  Thus, the mean total length adjusted to a common foot length was between \Sexpr{formatC(-1*confint(pos.lm2)["Popother",2],format="f",digits=2)} and \Sexpr{formatC(-1*confint(pos.lm2)["Popother",1],format="f",digits=2)} cm less for possums from other locations than for possums from Victoria.

\begin{table}[h]
  \centering
  \caption{ANOVA results from fitting the ultimate full model on the raw possum morphometric data.}\label{tab:IVROpTest1}
<<echo=FALSE, background="white">>=
kANOVA(pos.lm1)
@
\end{table}

The reduced model
\[ \mu_{totlngth|footlgth,Pop} = \alpha + \beta_{1}footlgth + \delta_{1}Popother  \]

can be used to predict the total length of an possum given a measurement of the possum's foot length.  For example, predicted total lengths for possums from both locations with foot lengths of 65 and 70 mm are shown in \tabref{tab:IVROpPreds1}.

\begin{table}[h]
  \centering
  \caption{Predicted total lengths, for the reduced model using all observations, for possums from both locations with foot lengths of 65 and 70 mm.}\label{tab:IVROpPreds1}
<<echo=FALSE, background="white">>=
fls <- c(65,65,70,70)
p1 <- predict(pos.lm2,data.frame(footlgth=fls,Pop=c("Vic","other","Vic","other")))
print(data.frame(footlgth=fls,p1),row.names=FALSE)
@
\end{table}

<<echo=FALSE>>=
 possum2 <- possum1[-c(31,39,41,43),]
 pos.lm3 <- lm(totlngth~footlgth*Pop,data=possum2)
@

Removal of the four individuals identified in assumption checking did not change the overall result of a parallel relationship ($p=0.4072$) with unequal-intercepts ($p<0.00005$; \figref{fig:IVROpFLP2}).  However, the common slope was estimated to be slightly (though likely not significantly) steeper at between 0.70 and 1.20 cm increase in total length for a 1 mm increase in foot length.  In addition, the difference in adjusted mean total lengths decreased slightly (though not significantly) to between 4.36 and 6.82 cm.  These results did not substantially alter the predictions for the 65 and 70 mm foot length individuals \tabrefp{tab:IVROpPreds2}.

<<IVROpFLP2, echo=FALSE, fig.scap="Fitted line plot from the fit of the ultimate model on the untransformed possum data with the four observation removed from the analysis.",fig.cap="Fitted line plot from the fit of the ultimate model on the untransformed possum data with the four observation flagged in \\figref{fig:IVROpRFLP1} removed from the analysis.  The dashed line represents the fitted line with the four individuals not removed.", fig.pos="h">>=
 fitPlot(pos.lm1,main="",xlab="Foot Length",ylab="Total Length",legend="topleft")
 abline(coef=c(coef(pos.lm3)[1],coef(pos.lm3)[2]),lty=2,lwd=2,col="blue")
@

\begin{table}[h]
  \centering
  \caption{Predicted total lengths, for the reduced model with four observations removed, for possums from both locations with foot lengths of 65 and 70 mm.}\label{tab:IVROpPreds2}
<<echo=FALSE, background="white">>=
pos.lm4 <- lm(totlngth~footlgth+Pop,data=possum2)
fls <- c(65,65,70,70)
p2 <- predict(pos.lm4,data.frame(footlgth=fls,Pop=c("Vic","other","Vic","other")))
print(data.frame(footlgth=fls,p2),row.names=FALSE)
@
\end{table}

\subsubsection*{Conclusion}
A significant relationship between total length and foot length was observed for possums collected near Victoria and those collected elsewhere.  The relationship was similar for possums collected at the two locations.  However, the intercepts were significantly different indicating that possums from Victoria had a significantly lower total length at every foot length.  Thus, possums from Victoria seemed to be smaller (in total length) even after differences in foot length sizes between the two locations were considered.

The original data contained four possums from Victoria that appeared unusual relative to other observations.  Three of these individuals had relatively small foot lengths and one individual had a much larger total length than would be expected given its foot length.  Removal of these four individuals, however, did not substantially change the results of this study.

\subsubsection*{Appendix -- R commands}
\begin{Verbatim}[formatcom=\color{red},xleftmargin=5mm,commandchars=\\\{\}]
> library(DAAG)
> data(possum)
> possum$Pop <- factor(possum$Pop,levels=c("Vic","other"))
> possum1 <- Subset(possum,!is.na(totlngth) & !is.na(footlgth),
+   select=c("Pop","totlngth","footlgth"))
> op2.lm1 <- lm(totlngth~footlgth*Pop,data=possum1)
> adTest(op2.lm1$residuals)
> outlierTest(op2.lm1)
> residPlot(op2.lm1,inclHist=FALSE)
> highlight(rstudent(op2.lm1)~op2.lm1$fitted.values,pts=c(31,39,41,43),col="black")
> fitPlot(op2.lm1,main="",xlab="Foot Length",ylab="Total Length",legend="topleft")
> highlight(totlngth~footlgth,data=possum1,pts=c(31,39,41,43),col="black")
> anova(op2.lm1)
> op2.lm2 <- lm(totlngth~footlgth+Pop,data=possum1)
> compIntercepts(op2.lm2)
> predictionPlot(op2.lm2,data.frame(footlgth=c(65,65,70,70),Pop=c("Vic","other","Vic","other")))

> possum2 <- possum1[-c(31,39,41,43),]
> op3.lm1 <- lm(totlngth~footlgth*Pop,data=possum2)
> adTest(op3.lm1$residuals)
> outlierTest(op3.lm1)
> residPlot(op3.lm1)
> fitPlot(op3.lm1,main="",xlab="Foot Length",ylab="Total Length",legend="topleft")
> anova(op3.lm1)
> op3.lm2 <- lm(totlngth~footlgth+Pop)
> compIntercepts(op3.lm2)
> predictionPlot(op3.lm2,data.frame(footlgth=c(65,65,70,70),Pop=c("Vic","other","Vic","other")))
\end{Verbatim}


%\vspace{144pt}
%\begin{center}
%\textbf{[ TURN THE PAGE ]}
%\end{center}

\newpage
\section{Summary Process}
The following is a template for a process of fitting a one-way IVR model.  Consider this process as you learn to fit one-way ANOVA models, but don't consider this to be a concrete process for all models.

\begin{Enumerate}
  \item Perform a thorough EDA.
    \begin{Itemize}
      \item Pay close attention to the form, strength, and outliers on the scatterplot of the response and explanatory variables separated by each level of the factor [\R{coplot()}]\footnote{The use of the \R{coplot()} function was not discussed in the chapter.  It's use for this purpose is rather simple and the output is fairly intuitive.  The \R{coplot()} function requires a model formula of the form \R{response}$\sim$\R{covariate | factor}.  The \R{|} symbol means ``conditioned on'' which can be interpreted as ``separated by.''}.
    \end{Itemize}
  \item Fit the untransformed ultimate full model [\R{lm()}].
  \item Check the assumptions of the fit of the model.
    \begin{Itemize}
      \item Check the linearity of the relationship with fitted-line [\R{fitPlot()}] and residual plots [\R{residPlot()}].
      \item Check homoscedasticity with fitted-line and residual plots.
      \item Check normality of residuals with a Anderson-Darling test [\R{adTest()}] and histogram of residuals [\R{residPlot()}].
      \item Check for outliers with the outlier test [\R{outlierTest()}].
    \end{Itemize}
  \item If an assumption or assumptions are violated then attempt to find a transformation where the assumptions are met.
    \begin{Itemize}
      \item Use the trial-and-error method [\R{transChooser()}], theory, or experience to identify possible transformations for the response and explanatory variables.
      \item If only an ``unusual'' or influential observation exists (i.e., linear, homoscedastic, and normal residuals) and no transformation corrects the ``problem'' then consider removing that observation from the data set.
      \item Fit the ultimate full model with the transformed variables or reduced data set.
    \end{Itemize}
  \item Construct an ANOVA table for the full model [\R{anova()}].  Interpret the F-test for the interaction between the covariate and the factor variable (i.e., the parallel lines test).
    \begin{Itemize}
      \item If the lines are NOT parallel then determine which slopes are different [\R{compSlopes()}].  SKIP to last step.
      \item If the lines are parallel then continue with next step.
    \end{Itemize}
  \item Interpret the F-test for the factor variable on the ANOVA table from the fit of the ultimate full model (i.e., equal-intercepts test).
    \begin{Itemize}
      \item If the intercepts are NOT equal then determine which mean adjusted values (i.e., intercepts) are different [\R{compIntercepts()}].  SKIP to last step
      \item If the intercepts are equal (i.e., coincident lines) then continue with next step.
    \end{Itemize}
  \item Interpret the F-test for the covariate on the ANOVA table from the fit of the ultimate full model to determine if there is a significant relationship between the response and the covariate.
  \item Summarize findings with a fitted-line plot.
    \begin{Itemize}
      \item Use the ultimate full model to construct the fitted-line plot but comment in your conclusion about significance of differences in slopes and intercepts.
    \end{Itemize}
\end{Enumerate}



\section{ANCOVA}
Analysis of covariance (ANCOVA) is a term that is used to describe linear models that contain both factor and quantitative explanatory variables \citep{Fox1997}.  Traditionally, the term ANCOVA has been restricted to the situation where it is assumed that the regression of the response on the explanatory variable has the same slope among levels of the factor variable.  With this definition, the ANCOVA model is a subset of the more general IVR models constructed in the previous sections of this chapter.  You should be aware when reading the literature because some authors use ANCOVA in the same way that IVR is used in this book.  However, in this book, an IVR will refer to all models that have both factor and quantitative explanatory models and ANCOVA will refer to all IVR models where it is assumed that the regression slopes are equal\footnote{That is, the model does not contain an interaction term among indicator variables and the quantitative explanatory variable.}.

\defn{ANCOVA model}{A subset of IVR models where the submodels have equal slopes.}

The derivation of the name -- analysis of covariance -- seems to stem from the fact that the quantitative explanatory variable is often called a \emph{covariate} and that the analysis is closely related to ANOVA.  In the latter regard, it should be noted that an ANCOVA is used to determine if the mean of a quantitative response variable differs among two or more groups defined by a factor variable.  However, an ANCOVA tests for the difference in means of the response variable among the groups AFTER adjusting for the relationship between the response variable and the covariate.

\defn{Covariate}{The quantitative explanatory variable in an IVR model.}

\vspace{-12pt}
\warn{An ANCOVA tests for difference in means of the response variable among levels of the factor variable after adjusting for the relationship between the response variable and the covariate.}

The process of an ANCOVA consists of two steps already discussed in previous sections.  First, it must be shown with a parallel lines test (see \sectref{sect:IVRPartialFTests}) that the slopes for all groups are equal.  If not, a strictly defined ANCOVA cannot be completed.  Second, if it is shown that the groups have parallel lines, then an equal-intercepts test (see \sectref{sect:IVRPartialFTests}) must be used to determine if the level means of the response variable adjusted for the covariate are all equal or not.  If not, then a Tukey's HSD test can be conducted to determine which adjusted means are significantly different.  The traditional ANCOVA uses means of the response variable that are adjusted to the mean of the covariate\footnote{The value that the response variables are adjusted to is not important when looking at the differences.  However, it may be important when presenting summaries (e.g., the mean) of the adjusted values.  To adjust the values to the mean of the covariate the \R{common.cov=} argument in the \R{compIntercepts()} function should be set to \R{mean(x)}, where $x$ is replaced with the name of the covariate.}.  The possum example above is an example of an ANCOVA because it was shown that the lines had equal slopes, whereas the crab example above is not an example of an ANCOVA because the regression slopes were unequal.

\warn{A traditional ANCOVA requires that all groups in the data have equal slopes.}

\vspace{-12pt}
\warn{A traditional ANCOVA adjusts the values of the response variable to the mean of the covariate.}

The ANCOVA works on the premise that if the relationship between the response variable and the covariate is represented by parallel lines for the groups, then the difference between the groups adjusted for this relationship is represented by the difference in intercepts between the groups.  In other words, if the groups have parallel lines but different intercepts, then the difference in the intercepts is a constant difference at all values of the covariate; thus, the difference in intercepts is the difference in the value of the response variable adjusted to any value of the covariate.

\subsection{ANCOVA in ecology}
An ANCOVA may be appropriate in any situation where the relationship between a quantitative explanatory variable and the quantitative response variable is thought to increase the variance of the response variable and, thus, mask real differences among the groups or to cause a separation among the groups that is not due to the groups themselves.  For example, researchers studying the effect of a treatment on the weight gain (i.e., response) of an animal may be worried that the initial size of the animals (i.e., covariate) results in an increase in variance of the weight gain.  In another example, researchers examining the number of eggs produced by a species of fish (i.e., response) in two lakes may be concerned that the observed difference in number of eggs is a result of one lake having larger fish (i.e., size is the covariate) than the other lake, rather than a ``real'' difference in number of eggs between the lakes.  Concerns such as these are so common in ecological and biological research that many researchers will compute a ratio of the response variable to the covariate\footnote{The use of ratios will be discussed in subsequent examples.  However, the use of ratios has been criticized or the use of ANCOVA is superior to ratios by \cite{LeCren1951}, \cite{Atchleyetal1976}, \cite{PackardBoardman1988}, \cite{Jacksonetal1990}, \cite{RaubenheimerSimpson1992}, \cite{Albrechtetal1993}, \cite{GarciaBerthouetal1993}, \cite{Raubenheimer1995}, \cite{BeaupreDunham1995}, and FISHERIES BOOK.} or construct residuals from a common line regression\footnote{The use of residuals was popularized by \cite{Jakob1996} but has been convincingly criticized by \cite{Maxwelletal1985}, \cite{HayesShonkwiler1996}, \cite{Smith1999}, and \cite{GarciaBerthoud2001}.} before using a one-way ANOVA to test for differences among the treatments or groups.  The fallacy of creating these ratios and the advantage of properly using an ANCOVA (or an IVR in general) is illustrated in this section with these two hypothetical situations.

\subsubsection*{Hypothetical Problem I}
Suppose that a researcher was interested in the effect of two temperatures on the mean weight gain of a particular type of fish.  Specifically, this researcher tested the following alternative hypothesis: $H_{A}: \mu_{8}\neq\mu_{10}$ where the subscripts represent temperatures.  The researcher measured the weight of each fish before the experiment began (hereafter, called initial body weight) so that fish of the same general size could be ``paired.''  One of each ``pair'' of fish was then randomly allocated to the $8^{o}$C treatment and the other was allocated to the $10^{o}$C treatment.  There were nine fish in each treatment for a total of 18 fish in the experiment.  After a prescribed period of time the weight gain of each fish was recorded (i.e., difference in weight from the beginning to the end of the experimental period).

It may seem reasonable that the effect of initial weight was ``handled'' by pairing fish of comparable size at the beginning of the experiment.  If this is believed, then the analyst would likely proceed with a one-way ANOVA\footnote{A two-sample t-test would also work in this situation because there are only two treatments.  Recall, thought, that it was shown in the \chapref{chap:LMANOVA1} that a one-way ANOVA for two groups is the same as a two-sample t-test.}.  A means plot (\figref{fig:IVRANCOVAEx1Plot1}-Left) and the one-way ANOVA \tabrefp{tab:IVRANCOVAEx1Res1} both indicate that there is no significant difference in weight gain between the two treatments.  However, if weight gain is plotted against initial weight (\figref{fig:IVRANCOVAEx1Plot1}-Right) then it is immediately seen that the fish in the $8^{o}$C treatment always gained more weight then the paired fish at the $10^{o}$C treatment.  Thus, the conclusion of no effect of temperature from the initial ANOVA analysis does not fit the intuitive conclusion made from examining the data in \figref{fig:IVRANCOVAEx1Plot1}-Right.

\begin{table}[h]
  \centering
  \caption{One-way ANOVA results for mean weight gain by temperature treatment.}\label{tab:IVRANCOVAEx1Res1}
<<echo=FALSE, background="white">>=
 d <- read.table("Figs/ANCOVA1.txt",header=TRUE)
 d$group <- factor(d$group,levels=c("8C","10C"))
 lma <- lm(wt.gain~group,data=d)
 kANOVA(lma)
@
\end{table}

<<IVRANCOVAEx1Plot1, echo=FALSE, fig.cap="Means plot for the mean weight gain by temperature treatment (Left) and scatterplot of weight gain versus initial weight by temperature treatment (Right).", fig.pos="h">>=
 grp.levs <- levels(d$group)
 fitPlot(lma,xlab="Temperature Treatment",ylab="Weight Gain",ylim=c(20,37),main="")
 plot(wt.gain~wt.init,data=d,pch=19,col=as.numeric(d$group),xlab="Initial Weight",ylab="Weight Gain",ylim=c(20,37))
 legend("topleft",legend=grp.levs,pch=19,col=c("black","red"))
@
\vspace{9pt}
The results from fitting the model
\[ \mu_{wt.gain|\cdots}=\alpha+\beta_{1}wt.init+\delta_{1}group10C+\gamma_{1}group10C*wt.init \]

showed that the submodel regression lines were parallel (parallel lines test p=0.9235) but the intercepts were not equal (equal-intercepts test p=0.0333; \figref{fig:IVRANCOVAEx1Plot2}-Left).  Tukey's HSD test estimated that the intercept for fish in the $10^{o}$C treatment was 1.77 g lower than the intercept for fish in the $8^{o}$C treatment with a 95\% confidence interval of 0.28 to 3.25 g.  Because the slopes are equal, one can also conclude that the adjusted means, i.e., the mean weight gain at the mean initial weight, are different by the same amounts (\figref{fig:IVRANCOVAEx1Plot2}-Right).

<<IVRANCOVAEx1Plot2, echo=FALSE, warning=FALSE, fig.cap="Fitted line plot for the regression of weight gain on initial weight by temperature treatment (Left) and means plot for the adjusted mean weight gain (at the mean initial weight) by temperature treatment (Right).", fig.pos="h">>=
 lm2 <- lm(wt.gain~wt.init+group,data=d)
 adj <- compIntercepts(lm2,common.cov=mean(d$wt.init))
 fitPlot(lm2,xlab="Initial Weight",ylab="Weight Gain",cex.main=0.8,legend="topleft",ylim=c(20,37),main="",col=c("black","red"))
 lm2a <- lm(adj$adjvals~group,data=d)
 fitPlot(lm2a,xlab="Temperature Treatment",ylab="Adjusted Weight Gain",ylim=c(20,37),main="")
@
\vspace{9pt}
This example illustrates how the inclusion of a ``strong'' covariate can mask the effects of a significant but ``weak'' factor variable.  In the initial analysis using just a one-way ANOVA on the weight gain variable, the increased variability due to the initial weight of the fish was not acknowledged and was thus left in the ``unexplained'' residual variability (i.e., $MS_{Residual}=32.63$; \tabref{tab:IVRANCOVAEx1Res1}).  The variability explained by the temperature treatment ($MS_{group}=14.04$; \tabref{tab:IVRANCOVAEx1Res1}) was very small relative to the unexplained variability and, thus, the temperature treatment was not considered significant.  However, when this large source of variability was explained by including the covariate in the ANCOVA, then the ``unexplained'' residual variability decreases dramatically (i.e., $MS_{Residual}=2.35$; \tabref{tab:IVRANCOVAEx1Res2}).  The variability explained by the temperature treatment remains the same but it now ``appears'' very large because the amount of variability unexplained is relatively small.  Thus, the temperature treatment becomes significant after the effect of initial weight on weight gain has been removed because the unexplained variability was significantly reduced.  In other words, in this example, the use of the ANCOVA helped identify a significant treatment effect that was masked by the increased variability due to the relationship between the response variable and a (previously ignored) covariate.  The reduction in residual variability can also be seen by the relatively ``tight'' confidence intervals for the adjusted means in \figref{fig:IVRANCOVAEx1Plot2}-Right relative to those for the unadjusted means in \figref{fig:IVRANCOVAEx1Plot1}-Left.

\begin{table}[h]
  \centering
  \caption{One-way ANCOVA results for mean weight gain adjusted by initial weight by temperature treatment.}\label{tab:IVRANCOVAEx1Res2}
<<echo=FALSE, background="white">>=
kANOVA(lm2)
@
\end{table}

\warn{An ANCOVA may increase the power to detect a difference among treatment means if a covariate is strongly related to the response variable.  In these instances a significant difference in treatment adjusted means may be detected when a difference in unadjusted treatment means was not detected.}

As mentioned previously, some researchers will attempt to remove the effect of the initial weight of the fish by computing a ratio of weight gain to initial weight.  This ratio can be thought of as a proportional growth measurement.  Then, to determine if there is a difference in proportional growth between the two treatments, the researchers will continue, under the assumption that the effect of the initial weight of the fish has been removed, with a one-way ANOVA on the proportional growth measurement.  However, a scatterplot of proportional growth versus initial weight shows that there is STILL a strong relationship between these two variables \figrefp{fig:IVRANCOVAEx1Plot3}.  Thus, the ``effect'' of initial weight has NOT been removed from the weight gain variable.  In addition, a one-way ANOVA on the proportional weight variable did not detect a difference in the mean of this ratio between the two treatments (p=0.1677) despite the rather obvious differences indicated in the scatterplot (i.e., the values for all fish kept at $8^{o}$C are higher than fish of the same initial size kept at $10^{o}$C).  It appears then, that this ratio did not completely remove the effect of the covariate and, thus, did not allow detection of the significant treatment effect present in the data.

<<IVRANCOVAEx1Plot3, echo=FALSE, fig.cap="Scatterplot of the ratio of the gain in body weight to the initial body weight versus initial body weight.", fig.pos="h">>=
 par(mar=c(3.5,5.0,1,1), mgp=c(2,0.75,0),las=1,tcl=-0.2)
 d$wt.ratio <- d$wt.gain/d$wt.init
 plot(wt.ratio~wt.init,data=d,xlab="Initial Weight",ylab=expression(frac("Weight Gain","Initial Weight")),col=as.numeric(d$group),pch=19)
 legend("topleft",legend=grp.levs,col=1:2,pch=19)
@

\warn{Contrived response variables that consists of the ratio between an observed response variable and a covariate do not completely remove the effect of the covariate.  An ANCOVA model should be used instead.}

\subsubsection*{Hypothetical Problem II}
In this example, suppose that a researcher was interested in determining if the gonad weight (g) differed between the same species of fish from two different locations.  In particular, the researcher tested the following statistical hypotheses: $H_{A}: \mu_{A}\neq\mu_{B}$ where the subscripts represent locations.  The researcher measured the body weight with the gonads removed (called somatic weight) and the gonad weight of nine randomly selected fish from each location.

A one-way ANOVA provides strong evidence for a difference in gonad weight between the two lakes (\tabref{tab:IVRANCOVAEx2Res1}; \figref{fig:IVRANCOVAEx2Plot1}-Left).  However, a plot of gonad weight versus somatic weight (\figref{fig:IVRANCOVAEx2Plot1}-Right) indicates that there is also a very strong difference in somatic weight between the two locations; indeed every fish at location B is larger then every fish at location A.  It is also clear from this plot that there is a very strong relationship between gonad weight and somatic weight.  Thus, it may be possible that the observed difference in gonad weight is mostly due to the differences observed in the somatic weight covariate.

\begin{table}[h]
  \centering
  \caption{One-way ANOVA results for mean gonad weight by location.}\label{tab:IVRANCOVAEx2Res1}
<<echo=FALSE, background="white", warning=FALSE>>=
 loc.levs <- levels(d$loc)
 lma <- lm(wt.gonad~loc,data=d)
 lm1 <- lm(wt.gonad~wt.somat,data=d)
 lm2 <- lm(wt.gonad~wt.somat+loc,data=d)
 lm3 <- lm(wt.gonad~wt.somat*loc,data=d)
 adj <- compIntercepts(lm2,common.cov=mean(d$wt.somat))
 kANOVA(lma)
@
\end{table}

<<IVRANCOVAEx2Plot1, echo=FALSE, fig.cap="Means plot for the mean gonad weight by location (Left) and scatterplot of gonad weight versus somatic weight by location (Right).", fig.pos="h">>=
 fitPlot(lma,xlab="Location",ylab="Gonad Weight",main="",ylim=c(22,56))
 plot(wt.gonad~wt.somat,data=d,pch=19,col=as.numeric(d$loc),xlab="Somatic Weight",ylab="Gonad Weight",ylim=c(22,56))
 legend("topleft",legend=loc.levs,pch=19,col=c("black","red"))
@

The results from fitting the model
\[ \mu_{wt.gonad|\cdots}=\alpha+\beta_{1}wt.somat+\delta_{1}locB+\gamma_{1}wt.somat*locB \]

showed that the submodel regression lines were parallel (parallel lines test p=0.8486) and the intercepts were equal (equal-intercepts test p=0.9309).  Thus, in this example, the two locations are fit by the same regression line (\figref{fig:IVRANCOVAEx2Plot2}-Left).  This result also indicates that the mean gonad weight adjusted to a common somatic weight does NOT differ between the two locations (\figref{fig:IVRANCOVAEx2Plot2}-Right).  The difference observed in the observed gonad weights was due to the difference in somatic weights and the strong relationship between gonad weight and somatic weight.

<<IVRANCOVAEx2Plot2, echo=FALSE, fig.cap="Fitted line plot for the regression of gonad weight on somatic weight by location (Left) and means plot for the adjusted mean gonad gain (at the mean somatic weight) by location (Right).", fig.pos="h">>=
 fitPlot(lm2,xlab="Somatic Weight",ylab="Gonad Weight",cex.main=0.8,legend="topleft",ylim=c(22,56),main="",col=c("black","red"))
 lm2a <- lm(adj$adjvals~loc,data=d)
 fitPlot(lm2a,xlab="Location",ylab="Adjusted Gonad Weight",ylim=c(22,56),main="")
@

This example illustrates how the inclusion of a ``strong'' covariate and a significant difference in the covariate among groups can make the ``weak'' effect of a factor variable appear significant.  In the case of the one-way ANOVA analysis, the variability that can be explained by the covariate was included in both the ``explained'' variability of the \var{loc} variable and the ``unexplained'' residual variability.  The portion included in the \var{loc} variable was very large making that variable appear very significant.  However, when the covariate was included in the ANCOVA analysis it was revealed that the SS due to the \var{loc} variable was negligible (=0.02; \tabref{tab:IVRANCOVAEx2Res2}).  Thus, nearly all of the SS attributable to \var{loc} in the one-way ANOVA results was really attributable to the covariate of somatic weight.  In other words, the use of the ANCOVA in this example helped properly attribute the cause of explained variability.

\begin{table}[h]
  \centering
  \caption{One-way ANCOVA results for mean gonad gain adjusted for somatic weight by location.}\label{tab:IVRANCOVAEx2Res2}
<<echo=FALSE, background="white">>=
kANOVA(lm2)
@
\end{table}

\warn{An ANCOVA may remove the effect of a covariate on a response variable that is confounding the effect of the factor on the response variable.  In these instances a significant difference in unadjusted treatment means may disappear when analyzing adjusted treatment means.}

This example also illustrates how the inclusion of a ``strong'' covariate substantially reduces the residual variance.  This is illustrated by the substantial reduction in $MS_{Residual}$ from \tabref{tab:IVRANCOVAEx2Res1} to \tabref{tab:IVRANCOVAEx2Res2} and the marked decrease in the width of confidence intervals from \figref{fig:IVRANCOVAEx2Plot1}-Left to \figref{fig:IVRANCOVAEx2Plot2}-Right.

In the fisheries literature it is very common to construct a contrived response variable called the gonadosomatic index (GSI).  The GSI is the ratio of gonad weight to somatic weight and is an attempt to remove the effect of somatic weight on gonad weight by putting the gonad weight on a ``per somatic weight'' basis.  A one-way ANOVA of the GSI variable, however, indicated that there was still a significant difference in the mean GSI between the two locations (p=0.0002).  However, a scatterplot of the GSI versus somatic weight shows that there is still a strong relationship between GSI and somatic weight (\figref{fig:IVRANCOVAEx2Plot3}).  In other words, once again, the effect of the covariate, somatic weight, on the response variable, gonad weight, has not been effectively factored out or removed.

<<IVRANCOVAEx2Plot3, echo=FALSE, fig.cap="Scatterplot of gonadosomatic index (GSI) versus somatic weight by location.", fig.pos="h">>=
 d$gsi <- d$wt.gonad/d$wt.somat
 plot(gsi~wt.somat,data=d,xlab="Somatic Weight",ylab="GSI",col=as.numeric(d$loc),pch=19)
 legend("topleft",legend=loc.levs,col=1:2,pch=19)
@

\subsubsection*{Advantages of ANCOVA}
The three major advantages of ANCOVA were illustrated in the two hypothetical examples above.  First, ANCOVA, in contrast to ratios of the response to the covariate, effectively removes the relationship between the response and the covariate. Second, the removal of this relationship, if a relationship exists, will reduce the residual error of the model.  In other words, more of the variability inherent in the response variable is explained.  This increases power and the likelihood of detecting a difference among treatments if a treatment really exists.  Third, the removal of the relationship between the covariate and the response allows the researcher to compare groups on an ``equal footing.'' In other words, the differences between the groups can be compared at a constant value of the covariate.

A main thing to remember about ANCOVA is that it is a very powerful method that is really only a subset of the models that have formed the bulk of this chapter.  An ANCOVA refers to the situation of describing the difference in intercepts when the slopes are equal.  The model fitting that formed the bulk of this chapter is not restricted to the case of equal slopes.

\vspace{36pt}
\begin{hwsection}{The answers to the first two questions can be hand-written (there is no R code required).  All remaining questions should be typed, refer to tables of output, be answered with complete sentences, and include an appendix of R commands.}

  \item \label{hwprob:LMIVRPronghorn1} \textbf{[10 pts]} In an effort to determine effective hand-raising protocols for pronghorn antelope (\emph{Antilocapra americana}), \cite{MartinParker1997} examined the growth rates (i.e., change in body weight over time) of two groups of pronghorns for the first 16 weeks of their lives.  The first group consisted of tame animals.  The second group originated from wild stock and was labeled ``diet-curtailed'' because their diet of artificial milk had to be curtailed to avoid problems with diarrhea.  Martin and Parker measured the body weight (kg) of each animal on various days during the 16 weeks.

    \begin{Enumerate}
      \item Construct an indicator variable such that the ``diet-curtailed'' group would be considered the reference group.
      \item Construct (i.e., write) the ultimate full model for an indicator variable regression with these data.
      \item Construct all of the possible submodels from your ultimate full model.
      \item Carefully interpret the meaning of each of the parameters in your ultimate full model.
      \item What are the models in the null and alternative hypothesis for the parallel lines test?
      \item What are the models in the null and alternative hypothesis for the intercepts test (assuming that the lines are parallel)?
    \end{Enumerate}

%\turnpage{36}
  \item \label{hwprob:LMIVRStomach1} \textbf{[10 pts]} \cite{PirhonenKoskela2005} tested the hypothesis that food intake was significantly correlated with stomach volume for rainbow trout (\emph{Oncorhynchus mykiss}).  If this correlation existed, then stomach volume could be predicted by estimating food intake rather than sacrificing and dissecting the fish.  Their experiment had four groups of fish -- those that had been starved for 1, 4, 8, or 16 days prior to being fed.  In one aspect of their study they wanted to determine if the model for predicting stomach volume from food intake differed by starvation length group.

    \begin{Enumerate}
      \item Construct indicator variable(s) such that the 1-day starved group would be considered the reference group.
      \item Construct (i.e., write) the ultimate full model for an indicator variable regression with these data.
      \item Construct all of the possible submodels from your ultimate full model.
      \item Carefully interpret the meaning of each of the parameters in your ultimate full model.
      \item What are the models in the null and alternative hypothesis for the parallel lines test?
      \item What are the models in the null and alternative hypothesis for the intercepts test (assuming that the lines are parallel)?
    \end{Enumerate}


  \item \label{hwprob:LMIVRBatMorph} \textbf{[15 pts]} \cite{Hutcheonetal2002} examined variations in total brain volume and in the volume of three brain regions (main olfactory bulb, hippocampus, auditory nuclei) using a data set for 63 species of bats (Chiroptera).  The data can be found in \dfile{Batmorph2.csv} (\href{https://github.com/droglenc/NCData/blob/master/Batmorph2.csv}{view}, \href{https://raw.githubusercontent.com/droglenc/NCData/master/Batmorph2.csv}{download}, \href{https://github.com/droglenc/NCData/blob/master/Batmorph2_meta.txt}{meta}).  They were primarily interested in determining if the volume of the brain region differed among broad foraging categories (phytophagous, gleaner, and aerial insectivore).  However, they decided to ``factor out'' the body weight of the bats because differences in size of bats among the foraging categories was suspected and thought to possibly mask possible differences in the volume of the brain regions.  A fourth foraging type -- vampire bats -- were excluded from the analysis (you should do this also).  Load these data into R and compute results to address the questions below assuming that interest is in just the \textbf{auditory nuclei mass}.

    \begin{Enumerate}
      \item Construct (i.e., write) the ultimate full model for an indicator variable regression with these data.  Use codes such that the phytophagous foraging group will be considered the reference group.
      \item Construct all of the possible submodels from your ultimate full model.
      \item Address all assumptions for the ultimate model.  If the major assumptions are not met then transform the data to a scale where the assumptions are met (and show that the assumptions are met).  Do not remove any individuals from the analysis.
      \item Perform a parallel lines test.  If the lines are not parallel then determine which pairs of lines differ.
      \item If appropriate, perform an intercepts test.  If the lines have different intercepts then determine which pairs of lines are different.
      \item Write an overall conclusion for these data.
    \end{Enumerate}

\end{hwsection}
